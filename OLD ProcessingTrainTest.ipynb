{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc2bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Install necessary packages ----------\n",
    "%pip install numpy pandas opencv-python scikit-learn matplotlib tqdm pillow timm pyyaml joblib \n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa8f3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hamad\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ---------- Import necessary libraries ----------\n",
    "import json, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28b151d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 5070 Ti\n",
      "CUDA is available! Training on GPU...\n"
     ]
    }
   ],
   "source": [
    "# ---------- Device setup and check GPU availability ----------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA is available! Training on GPU...\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Training on CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b88da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Variables Declaration ----------\n",
    "ROOT = Path(\".\")  # Project-Code root\n",
    "CSV_FALL = ROOT / \"FallDetectionLabels.csv\"\n",
    "CSV_GEST = ROOT / \"HandGestureLabels.csv\"\n",
    "\n",
    "DATASET_DIR = ROOT / \"Dataset\"\n",
    "OUT_FALL = ROOT / \"Processed Dataset\" / \"ProcessedFallDetection\"\n",
    "OUT_GEST = ROOT / \"Processed Dataset\" / \"ProcessedHandGesture\"\n",
    "\n",
    "PROCESSED_CSV_FALL = ROOT / \"ProcessedFallDetection.csv\"\n",
    "PROCESSED_CSV_GEST = ROOT / \"ProcessedHandGesture.csv\"\n",
    "\n",
    "SAMPLE_EVERY = 1\n",
    "OUT_SIZE = 600  # EfficientNet-B7 scale\n",
    "OVERWRITE = False\n",
    "VIDEO_EXTS = [\".mp4\", \".avi\", \".mov\", \".mkv\", \".MP4\", \".AVI\", \".MOV\", \".MKV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d45855f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 4748.39it/s]\n",
      "100%|██████████| 160/160 [00:00<00:00, 4338.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Video preprocessing script to convert videos to motion images\n",
    "\n",
    "# ---------- utils ----------\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# resolve video path function\n",
    "def resolve_video_path(rel_path: str) -> Path:\n",
    "    \"\"\"Resolve a relative CSV path to an actual video file.\n",
    "    Accepts with or without extension, or a dir containing a single video.\"\"\"\n",
    "    p = ROOT / rel_path\n",
    "    if p.is_file():\n",
    "        return p\n",
    "\n",
    "    # try adding common extensions if no suffix\n",
    "    if p.suffix == \"\":\n",
    "        for ext in VIDEO_EXTS:\n",
    "            cand = p.with_suffix(ext)\n",
    "            if cand.is_file():\n",
    "                return cand\n",
    "\n",
    "    # if it's a directory, pick the first known video\n",
    "    if p.is_dir():\n",
    "        for ext in VIDEO_EXTS:\n",
    "            vids = sorted(p.glob(f\"*{ext}\"))\n",
    "            if vids:\n",
    "                return vids[0]\n",
    "\n",
    "    raise FileNotFoundError(f\"Video not found for entry: {rel_path}\")\n",
    "\n",
    "# pad image to square function\n",
    "def pad_to_square(img: np.ndarray) -> np.ndarray:\n",
    "    h, w = img.shape[:2]\n",
    "    s = max(h, w)\n",
    "    top = (s - h) // 2\n",
    "    bottom = s - h - top\n",
    "    left = (s - w) // 2\n",
    "    right = s - w - left\n",
    "    return cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=0)\n",
    "\n",
    "# main video processing function, video to motion image\n",
    "def process_video_to_motion_image(video_path: Path, sample_every: int = 5, out_size: int = 600) -> np.ndarray:\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Failed to open {video_path}\")\n",
    "\n",
    "    sampled = []\n",
    "    idx = 0\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "        if idx % sample_every == 0:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
    "            sampled.append(gray)\n",
    "        idx += 1\n",
    "    cap.release()\n",
    "\n",
    "    if len(sampled) < 2:\n",
    "        raise RuntimeError(f\"Not enough sampled frames ({len(sampled)}) in {video_path}\")\n",
    "\n",
    "    acc = np.zeros_like(sampled[0], dtype=np.float32)\n",
    "    prev = sampled[0]\n",
    "    for cur in sampled[1:]:\n",
    "        diff = np.abs(cur - prev)\n",
    "        thr = np.percentile(diff, 99) # threshold to reduce noise\n",
    "        mask = diff >= thr\n",
    "        acc[mask] += diff[mask]\n",
    "        prev = cur\n",
    "\n",
    "    m = acc.max()\n",
    "    if m > 0:\n",
    "        acc = acc / m\n",
    "    acc = (acc * 255.0).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    acc = pad_to_square(acc)\n",
    "    acc = cv2.resize(acc, (out_size, out_size), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # stack grayscale to 3 channels for EfficientNet\n",
    "    img3 = np.stack([acc, acc, acc], axis=2)\n",
    "    return img3\n",
    "\n",
    "# output path mapping function\n",
    "def target_path_for_output(out_root: Path, rel_video_path: str, ext: str = \".png\") -> Path:\n",
    "    \"\"\"Write directly under the processed root without an extra Dataset/HandGesture or Dataset/FallDetection level.\n",
    "    Example:\n",
    "      Dataset/HandGesture/CurtainGesture/Hamad/CGH1.mp4\n",
    "      -> OUT_GEST/CurtainGesture/Hamad/CGH1.png\n",
    "    \"\"\"\n",
    "    rel = Path(rel_video_path)\n",
    "    parts = list(rel.parts)\n",
    "\n",
    "    # strip leading 'Dataset'\n",
    "    if parts and parts[0].lower() == \"dataset\":\n",
    "        parts = parts[1:]\n",
    "\n",
    "    # also strip the next level if it is HandGesture or FallDetection\n",
    "    if parts and parts[0].lower() in (\"handgesture\", \"falldetection\"):\n",
    "        parts = parts[1:]\n",
    "\n",
    "    rel_no_ext = Path(*parts).with_suffix(ext)\n",
    "    return out_root / rel_no_ext\n",
    "\n",
    "# process a dataframe table\n",
    "def process_table(df: pd.DataFrame, out_root: Path, has_user: bool) -> pd.DataFrame:\n",
    "    records, failures = [], []\n",
    "\n",
    "    for row in tqdm(df.itertuples(index=False), total=len(df)):\n",
    "        rel_path = getattr(row, \"video_path\")\n",
    "        label = getattr(row, \"label\")\n",
    "        user_id = getattr(row, \"user_id\") if has_user else None\n",
    "\n",
    "        try:\n",
    "            abs_video = resolve_video_path(rel_path)\n",
    "            out_img = target_path_for_output(out_root, rel_path, ext=\".png\")\n",
    "            ensure_dir(out_img.parent)\n",
    "\n",
    "            if OVERWRITE or not out_img.exists():\n",
    "                img = process_video_to_motion_image(abs_video, sample_every=SAMPLE_EVERY, out_size=OUT_SIZE)\n",
    "                ok = cv2.imwrite(str(out_img), cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "                if not ok:\n",
    "                    raise RuntimeError(\"cv2.imwrite failed\")\n",
    "\n",
    "            rel_img = out_img.relative_to(ROOT).as_posix()\n",
    "            rec = {\"image_path\": rel_img, \"label\": label, \"video_path\": rel_path}\n",
    "            if has_user:\n",
    "                rec[\"user_id\"] = user_id\n",
    "            records.append(rec)\n",
    "\n",
    "        except Exception as e:\n",
    "            failures.append({\"video_path\": rel_path, \"label\": label, \"error\": str(e)})\n",
    "\n",
    "    if failures:\n",
    "        fail_csv = out_root / \"processing_failures.csv\"\n",
    "        pd.DataFrame(failures).to_csv(fail_csv, index=False)\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# ---------- run preprocessing ----------\n",
    "ensure_dir(OUT_FALL)\n",
    "ensure_dir(OUT_GEST)\n",
    "\n",
    "if CSV_FALL.exists():\n",
    "    df_fall = pd.read_csv(CSV_FALL)\n",
    "    out_fall_df = process_table(df_fall, OUT_FALL, has_user=False)\n",
    "    out_fall_df.to_csv(PROCESSED_CSV_FALL, index=False)\n",
    "else:\n",
    "    print(f\"Missing CSV: {CSV_FALL}\")\n",
    "\n",
    "if CSV_GEST.exists():\n",
    "    df_gest = pd.read_csv(CSV_GEST)\n",
    "    out_gest_df = process_table(df_gest, OUT_GEST, has_user=True)\n",
    "    out_gest_df.to_csv(PROCESSED_CSV_GEST, index=False)\n",
    "else:\n",
    "    print(f\"Missing CSV: {CSV_GEST}\")\n",
    "\n",
    "print(\"Preprocessing done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9478c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Variable Declaration ----------\n",
    "FEATURES_ROOT = ROOT / \"Features\" / \"EffLite0\"  \n",
    "OUT_FEAT_FALL_DIR = FEATURES_ROOT / \"FallDetection\"\n",
    "OUT_FEAT_GEST_DIR = FEATURES_ROOT / \"HandGesture\"\n",
    "MAP_CSV_FALL = ROOT / \"FallDetectionFeaturesEffLite0.csv\"\n",
    "MAP_CSV_GEST = ROOT / \"HandGestureFeaturesEffLite0.csv\"\n",
    "\n",
    "FEATURE_DIM = 1280\n",
    "BATCH_SIZE = 2\n",
    "NUM_WORKERS = 0\n",
    "OVERWRITE_FEATURES = False  # set False to skip existing\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ac3d74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 17.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: FallDetectionFeaturesEffLite0.csv | Skipped existing: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:01<00:00, 63.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: HandGestureFeaturesEffLite0.csv | Skipped existing: 160\n",
      "Feature extraction finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Feature extraction with EfficientNet-B7 (timm) ---\n",
    "\n",
    "# Model (separate name to avoid clashing with later training models)\n",
    "# feat_model = timm.create_model(\"tf_efficientnet_b7\", pretrained=True, num_classes=0, global_pool=\"avg\")  # 2560-d\n",
    "feat_model = timm.create_model(\"tf_efficientnet_lite0\", pretrained=True, num_classes=0, global_pool=\"avg\") # 1280-d\n",
    "feat_model.eval().to(device)\n",
    "\n",
    "# Images are already 600x600, 3ch; no resize here\n",
    "extract_tform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # HWC uint8 -> CHW float in [0,1]\n",
    "    transforms.Resize((320, 320)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Processed Dataset class\n",
    "class ProcessedImageDataset(Dataset):\n",
    "    def __init__(self, csv_path: Path, root: Path, has_user: bool):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.root = root\n",
    "        self.has_user = has_user\n",
    "        self.has_user_col = \"user_id\" in self.df.columns\n",
    "        self.has_video_col = \"video_path\" in self.df.columns\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_rel = row[\"image_path\"]\n",
    "        img_abs = self.root / img_rel\n",
    "        bgr = cv2.imread(str(img_abs), cv2.IMREAD_COLOR)\n",
    "        if bgr is None:\n",
    "            raise FileNotFoundError(f\"Missing image: {img_abs}\")\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        x = extract_tform(rgb)\n",
    "\n",
    "        label = row[\"label\"]\n",
    "        user = row[\"user_id\"] if (self.has_user and self.has_user_col) else \"\"\n",
    "        vpath = row[\"video_path\"] if self.has_video_col else \"\"\n",
    "        return x, img_rel, label, user, vpath\n",
    "\n",
    "# path mapping function\n",
    "def feature_path_for_image(out_root: Path, image_rel_path: str) -> Path:\n",
    "    \"\"\"\n",
    "    Map processed image path to features root, removing:\n",
    "      'Processed Dataset' and the dataset-specific level.\n",
    "    \"\"\"\n",
    "    rel = Path(image_rel_path)\n",
    "    parts = list(rel.parts)\n",
    "    if parts and parts[0].lower() == \"processed dataset\":\n",
    "        parts = parts[1:]\n",
    "    if parts and parts[0].lower() in (\"processedhandgesture\", \"processedfalldetection\"):\n",
    "        parts = parts[1:]\n",
    "    rel_no_ext = Path(*parts).with_suffix(\".npy\")\n",
    "    return out_root / rel_no_ext\n",
    "\n",
    "# feature extraction function\n",
    "@torch.no_grad()\n",
    "def extract_and_save_features(csv_in: Path, out_dir: Path, map_csv_out: Path, has_user: bool):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    FEATURES_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ds = ProcessedImageDataset(csv_in, ROOT, has_user=has_user)\n",
    "    g = torch.Generator(device=\"cpu\"); g.manual_seed(SEED)\n",
    "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=True, generator=g)\n",
    "\n",
    "    records, skipped = [], 0\n",
    "\n",
    "    for xb, img_rels, labels, users, vpaths in tqdm(loader, total=len(loader)):\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        with torch.amp.autocast(\"cuda\", enabled=(device.type == \"cuda\")):\n",
    "            feats = feat_model(xb)  # [B, 2560]\n",
    "        feats = feats.float().cpu().numpy()\n",
    "\n",
    "        for i in range(len(img_rels)):\n",
    "            img_rel = img_rels[i]\n",
    "            feat_path = feature_path_for_image(out_dir, img_rel)\n",
    "            feat_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            if not OVERWRITE_FEATURES and feat_path.exists():\n",
    "                skipped += 1\n",
    "            else:\n",
    "                np.save(feat_path, feats[i])\n",
    "\n",
    "            rec = {\n",
    "                \"image_path\": img_rel,\n",
    "                \"feature_path\": feat_path.relative_to(ROOT).as_posix(),\n",
    "                \"label\": labels[i],\n",
    "            }\n",
    "            if has_user:\n",
    "                rec[\"user_id\"] = users[i]\n",
    "            if vpaths[i]:\n",
    "                rec[\"video_path\"] = vpaths[i]\n",
    "            records.append(rec)\n",
    "\n",
    "    pd.DataFrame(records).to_csv(map_csv_out, index=False)\n",
    "    print(f\"Saved: {map_csv_out} | Skipped existing: {skipped}\")\n",
    "\n",
    "# ---- run both datasets (uses existing PROCESSED_CSV_* vars) ----\n",
    "if PROCESSED_CSV_FALL.exists():\n",
    "    extract_and_save_features(PROCESSED_CSV_FALL, OUT_FEAT_FALL_DIR, MAP_CSV_FALL, has_user=False)\n",
    "else:\n",
    "    print(f\"Missing {PROCESSED_CSV_FALL}\")\n",
    "\n",
    "if PROCESSED_CSV_GEST.exists():\n",
    "    extract_and_save_features(PROCESSED_CSV_GEST, OUT_FEAT_GEST_DIR, MAP_CSV_GEST, has_user=True)\n",
    "else:\n",
    "    print(f\"Missing {PROCESSED_CSV_GEST}\")\n",
    "\n",
    "print(\"Feature extraction finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4d999595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in mapping: 100\n",
      "Missing files: 0\n",
      "Bad shapes: 100  expected (1280,)\n",
      "NaN or Inf vectors: 0\n",
      "Example bad shape: [('Features/MNASNet/FallDetection/Falling/FDF1.npy', (1000,)), ('Features/MNASNet/FallDetection/Falling/FDF2.npy', (1000,)), ('Features/MNASNet/FallDetection/Falling/FDF3.npy', (1000,))]\n",
      "Image: Processed Dataset/ProcessedFallDetection/Standing/FDSD4.png\n",
      "Label: fall\n",
      "Feature shape: (1000,)\n",
      "First 20 values: [-0.4622 -0.4451 -0.0923  0.3157  0.3604 -0.3757  0.0029  0.6909  0.6318\n",
      " -0.3213 -0.0458 -0.2656 -0.1851 -0.3164 -0.3726 -0.0828 -0.2485 -0.2676\n",
      " -0.303  -0.3064]\n",
      "Stats min max mean std: -0.99072265625 2.083984375 0.0001270542124984786 0.4756785035133362\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHHCAYAAACle7JuAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAL5xJREFUeJzt3Ql4VFWa//E3IatAgmFJQNlBQEA2kUUa2SQizYAgCNIOIA0tE7CBVpYZgQbRgCIwKIg6AtqtoqjgwghiZGmRHVRwiayCQoIiCZuEJXee9/yfqn9VyG6SqlP5fp7nktS9t26de6qo+uUst4Icx3EEAADAQsG+LgAAAEBhEWQAAIC1CDIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKxFkAEAANYiyAAAAGsRZAD4xLJlyyQoKEiOHDki/qJWrVoydOhQ9+0NGzaYMurP4vb3v//dPJYnvT169Ggprc8HkB8EGSCPN/bslkmTJhXLY37++efmAy0tLa1Yjo+S8eSTT8qqVavEH/lz2YDCCCnUvYBSZMaMGVK7dm2vdU2aNCm2IDN9+nTTKlChQoVieQzkX8eOHeW3336TsLCwAoeFe++9V/r06ZPv+zz22GPFFpDzU7YHHnhABg4cKOHh4cVeBqAoEWSAPPTo0UNuvfVWsdn58+elbNmyvi6GdYKDgyUiIqJEnpuQkBCz+EqZMmXMAtiGriXgd/roo4/kD3/4g/kwKl++vPTs2VO+/vprr32++uor08pSp04d88EYFxcnDz74oJw6dcq9j3YpPfroo+Z3bQFydWPpmAVd9Hft7spK1+t9PY+j67755hu5//775frrr5cOHTq4t//zn/+UVq1aSWRkpMTExJi/wo8dO5brOb799tvmmBs3brxm2wsvvGC27du3L9/nmpOs55LT2BWl3W9jx46V6tWrm1aEevXqyezZsyUzMzPPx3EcR2bOnCk33nijXHfdddK5c+drnrOcxsjs379f+vXrZ85Lz0+PoXWYnp7uPgcNJ6+88or7OXSVPbfnJrsxMi6vvfaaNGjQwDyePnebNm3y2q7H1zrKKusxcytbTmNkFi1aJI0bNzZ1XK1aNUlISLim67NTp06mlVLPS+tS6/SGG26Qp556Ks/nAvi9aJEB8qAfUL/88ovXukqVKpmf//jHP2TIkCESHx9vPkQvXLggzz//vPlw2rNnj/vDZd26dXLo0CEZNmyY+QDUD80XX3zR/Ny6dav5AOnbt698//338sYbb8i8efPcj1G5cmX5+eefC1zu/v37S/369U1Xgn5wqyeeeEKmTJkiAwYMkD//+c/muM8++6zpQtHy5tSdpeGsXLly8tZbb8kdd9zhte3NN980H3Su7rb8nOvvpfWs5fjpp5/kL3/5i9SoUcN0y02ePFlOnDgh8+fPz/X+U6dONUHm7rvvNsvu3bule/fucunSpVzvp9v1uc7IyJAxY8aY89MyfPjhh+bDPTo62rwmtG5vu+02GTlypLlf3bp183xucqLhUev44YcfNmFCg8Vdd90l27dvL3AXZ37KljUIaVdnt27dZNSoUZKcnGxe3zt27JDNmzdLaGioe9/Tp0+bcunrWF9fGn4nTpwoTZs2Na2aQLFxAGRr6dKl+gmT7aLOnj3rVKhQwRkxYoTX/VJSUpzo6Giv9RcuXLjm+G+88YY51qZNm9zrnn76abPu8OHDXvvqbV2vZcpK10+bNs19W3/XdYMGDfLa78iRI06ZMmWcJ554wmv93r17nZCQkGvWZ6XHq1KlinPlyhX3uhMnTjjBwcHOjBkzCnyurvr1PNes5+JSs2ZNZ8iQIe7bjz/+uFO2bFnn+++/99pv0qRJ5hyPHj2a43mcPHnSCQsLc3r27OlkZma61//nf/6neXzPx1m/fr1Zpz/Vnj17zO0VK1bkUlOOKZvncfJ6bjy3eXK93nbu3Ole98MPPzgRERHOPffc416nj6V1lJ9j5lS2rM+Hq566d+/uXL161b3fc889Z/ZbsmSJe90dd9xh1r366qvudRkZGU5cXJzTr1+/HGoJKBp0LQF5WLhwoWll8FyU/tS/wgcNGmRabFyLjjNo06aNrF+/3n0M7cZxuXjxotmvbdu25ra2BhSHhx56yOv2u+++a7pd9K9lz/Jqq4K2DniWNzv33XefnDx50qubRf/q1mPqtpI81xUrVpjuPO2a8TwXbTm4evXqNV0vnj755BPTsqItKp6tQ9pNlRdtcVFr1641rUJF9dzkpl27dqY7yUVbn3r37m3KoOdaXFz1pPWiY4VcRowYIVFRUbJ69Wqv/bXF7k9/+pP7tg6Q1pYfbZ0DihNdS0Ae9M04u8G+OlZCdenSJdv76Zu9y6+//mqa6JcvX27CgCfX2IqilnWmlZZX/8jX0JIdz26C7Gi3gX6QazdH165dzTr9vXnz5nLTTTeV6LnquehYHO12y07Wx/X0ww8/mJ9Z60GPpcEorzodP368zJ0714xb0TD1b//2b+YD3BVyCvPc5Ca750vrW4OUdg1qEC0OrnrSsTmeNKDo+CfXdhcdK5S121DrU58noDgRZIBCcg0q1XEH2X2YeM5A0VYQHcOhg3n1g1//etX7azjIz+DUnMaV5PYXuWfLiKu8ehwdnJzd7BQtU250fIZO2V25cqUZp5GammrGSeg4D0+/91zzc556nDvvvFMmTJiQ7f6ewaqoPfPMM2aA7HvvvScff/yxGbuSmJhoxv/oh3l+ZH1ufq/CvD6KWk4znvIaAwT8XgQZoJBcgySrVKliujRyooMgk5KSTCuFDjLN2qKTnw8kV0tB1tkiWf8qzqu8+qGirQGF/aDXLiSd8aLn8+2335rjeXYrFeRcczrPrOeo3Rs6gDfruZw7dy7Xes9JzZo13WXSlgUXbd3Q8ueHDmDVRa/9oqHt9ttvl8WLF5sBxKooBjTnVnc6KFxnBrlapLKrt5xeH/ktm6uedICvZz3p83H48OFC1T1QHBgjAxSSzl7R7iNtkbh8+fI1210zjVx/qWb9yzS7mTWua71k/VDSx9FZTFnHfmjLSH7pbBIti4aMrGXR2/mZHq0fXjplW7uUdNFuN89ukoKca3Y0oGQ9R53xlLVlQVt9tmzZYsaJZKV1d+XKlVzPQbvRdLaWZznzU8YzZ85cc2wNNDqGRGcyeT6PRXV1Zj1Pz7FFOlVeW4N0lpWrvrXetNvOsxtHw5+2nmWV37JpPWk30oIFC7zq6eWXXzaPpTPZAH9AiwxQSBoudCqqXhG1ZcuW5loi+hfy0aNHzUBI/Sv9ueeeM/vp9Ga9poYGHr2+hnZJ6F+1WbkGdf7Xf/2XOZ5+4Pbq1ct8+Oi02VmzZpmfOmZHP/D1L/P80g87bTHQKcp6rRDtJtLr3mg59ANPp+M+8sgjuR5Dy6OBSMe/6PVI5syZc02d5Pdcs6PnpgNh9Tot2nX05ZdfmrDimoruot1W77//vvzxj3803Txab1qevXv3mgHIen5Z7+Oiz5Gep3YH6f11+rVOPdcut5zu4/Lpp5+a7z7S6dPaqqWhRrsWNVBomV20PDpYVsfS6LVXNOzpAPDC0CnWGpo9p18rDaQu+lrRqc733HOP2c91GQAtY9YB1vktm9aTvlb0cbRbUMcCaeuMPn7r1q29BvYCPlVEs5+AgOOajrpjx45c99OpufHx8WbKtU6LrVu3rjN06FCvKbM//vijmS6r07V1v/79+zvHjx/PdrqxTi2+4YYbzLRmz+mwOq15+PDh5v7ly5d3BgwYYKbI5jT9+ueff862vO+8847ToUMHMw1Xl4YNGzoJCQlOcnJyvupl3bp15vhBQUHOsWPHrtme33PNbvq1TvOdOHGiU6lSJee6664z9XrgwIFrpl+7pr9PnjzZqVevnpkmrPdp3769M2fOHOfSpUu5noM+zvTp052qVas6kZGRTqdOnZx9+/Zd8zhZp18fOnTIefDBB81zrM91TEyM07lzZ+eTTz7xOv53333ndOzY0Rzbc0p3bs9NTtOv9bn55z//6dSvX98JDw93WrRo4S6Pp48//thp0qSJqYsGDRqY+2R3zJzKlt3z4Zpura+R0NBQJzY21hk1apRz+vRpr310+nXjxo2vKVNO08KBohSk//g2SgEAABQOY2QAAIC1CDIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKwV8BfE0+9kOX78uLnwV1FeNhwAABQfvTrM2bNnzYUbPb+BvdQFGQ0x1atX93UxAABAIejXcuT2hawBH2S0JcZVEXr5dAAA4P/0u820IcL1OV5qg4yrO0lDDEEGAAC75DUshMG+AADAWgQZAABgLYIMAACwFkEGAABYiyADAACsRZABAADWIsgAAABrEWQAAIC1CDIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKxFkAEAANYiyAAAAGuF+LoAQKCqNWl1nvscmdWzRMoCAIGKFhkAAGAtggwAALAWQQYAAFiLIAMAAKzFYF9Ywd8GzuanPACA4keLDAAAsBZBBgAAWIsgAwAArEWQAQAA1iLIAAAAaxFkAACAtXweZH766Sf505/+JBUrVpTIyEhp2rSp7Ny5073dcRyZOnWqVK1a1Wzv1q2b7N+/36dlBgAA/sGnQeb06dNy++23S2hoqHz00UfyzTffyDPPPCPXX3+9e5+nnnpKFixYIIsXL5Zt27ZJ2bJlJT4+Xi5evOjLogMAgNJ+QbzZs2dL9erVZenSpe51tWvX9mqNmT9/vjz22GPSu3dvs+7VV1+V2NhYWbVqlQwcONAn5QYAAP7Bpy0y77//vtx6663Sv39/qVKlirRo0UJeeukl9/bDhw9LSkqK6U5yiY6OljZt2siWLVuyPWZGRoacOXPGawEAAIHJp0Hm0KFD8vzzz0v9+vVl7dq1MmrUKHn44YfllVdeMds1xChtgfGkt13bskpMTDRhx7Voiw8AAAhMPg0ymZmZ0rJlS3nyySdNa8zIkSNlxIgRZjxMYU2ePFnS09Pdy7Fjx4q0zAAAwH/4NMjoTKSbb77Za12jRo3k6NGj5ve4uDjzMzU11Wsfve3allV4eLhERUV5LQAAIDD5NMjojKXk5GSvdd9//73UrFnTPfBXA0tSUpJ7u4550dlL7dq1K/HyAgAA/+LTWUvjxo2T9u3bm66lAQMGyPbt2+XFF180iwoKCpKxY8fKzJkzzTgaDTZTpkyRatWqSZ8+fXxZdAAAUNqDTOvWrWXlypVmXMuMGTNMUNHp1oMHD3bvM2HCBDl//rwZP5OWliYdOnSQNWvWSEREhC+LDgAASnuQUX/84x/NkhNtldGQowsAAIBffUUBAABAYRFkAACAtQgyAADAWgQZAABgLYIMAACwFkEGAABYiyADAACsRZABAADWIsgAAABrEWQAAIC1CDIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKxFkAEAANYiyAAAAGsRZAAAgLUIMgAAwFoEGQAAYC2CDAAAsFaIrwsAIHe1Jq3Oc58js3qWSFkAwN/QIgMAAKxFkAEAANYiyAAAAGsRZAAAgLUIMgAAwFoEGQAAYC2CDAAAsBZBBgAAWIsgAwAArEWQAQAA1iLIAAAAaxFkAACAtQgyAADAWgQZAABgLYIMAACwFkEGAABYiyADAACsRZABAADWIsgAAABrEWQAAIC1CDIAAMBaBBkAAGAtggwAALCWT4PM3//+dwkKCvJaGjZs6N5+8eJFSUhIkIoVK0q5cuWkX79+kpqa6ssiAwAAP+LzFpnGjRvLiRMn3Mtnn33m3jZu3Dj54IMPZMWKFbJx40Y5fvy49O3b16flBQAA/iPE5wUICZG4uLhr1qenp8vLL78sr7/+unTp0sWsW7p0qTRq1Ei2bt0qbdu29UFpAQCAP/F5i8z+/fulWrVqUqdOHRk8eLAcPXrUrN+1a5dcvnxZunXr5t5Xu51q1KghW7ZsyfF4GRkZcubMGa8FAAAEJp+2yLRp00aWLVsmDRo0MN1K06dPlz/84Q+yb98+SUlJkbCwMKlQoYLXfWJjY822nCQmJprjoPSpNWl1kRznyKyeUlrrJ1DPHUDg8mmQ6dGjh/v3W265xQSbmjVryltvvSWRkZGFOubkyZNl/Pjx7tvaIlO9evUiKS8AAPAvPu9a8qStLzfddJMcOHDAjJu5dOmSpKWlee2js5ayG1PjEh4eLlFRUV4LAAAITH4VZM6dOycHDx6UqlWrSqtWrSQ0NFSSkpLc25OTk80Ymnbt2vm0nAAAwD/4tGvpkUcekV69epnuJJ1aPW3aNClTpowMGjRIoqOjZfjw4aabKCYmxrSsjBkzxoQYZiwBAACfB5kff/zRhJZTp05J5cqVpUOHDmZqtf6u5s2bJ8HBweZCeDobKT4+XhYtWsQzBwAAfB9kli9fnuv2iIgIWbhwoVkAAAD8eowMAABAQRBkAACAtQgyAADAWgQZAABgLYIMAACwFkEGAABYiyADAACsRZABAADWIsgAAABrEWQAAIC1CDIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKxFkAEAANYK8XUBENhqTVrt6yIAAAIYLTIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKxFkAEAANYiyAAAAGsRZAAAgLUIMgAAwFoEGQAAYC2+ogAIAEX1VRD5Oc6RWT2L5LEAoCjQIgMAAKxFkAEAANYiyAAAAGsRZAAAgLUY7AsU08BZAEDxo0UGAABYiyADAACsRZABAADWIsgAAABrEWQAAIC1CDIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKxFkAEAANYiyAAAAGv5TZCZNWuWBAUFydixY93rLl68KAkJCVKxYkUpV66c9OvXT1JTU31aTgAA4D/8Isjs2LFDXnjhBbnlllu81o8bN04++OADWbFihWzcuFGOHz8uffv29Vk5AQCAf/F5kDl37pwMHjxYXnrpJbn++uvd69PT0+Xll1+WuXPnSpcuXaRVq1aydOlS+fzzz2Xr1q0+LTMAAPAPPg8y2nXUs2dP6datm9f6Xbt2yeXLl73WN2zYUGrUqCFbtmzJ8XgZGRly5swZrwUAAASmEF8++PLly2X37t2maymrlJQUCQsLkwoVKnitj42NNdtykpiYKNOnTy+W8gIAAP/isxaZY8eOyV//+ld57bXXJCIiosiOO3nyZNMt5Vr0cQAAQGDyWZDRrqOTJ09Ky5YtJSQkxCw6oHfBggXmd215uXTpkqSlpXndT2ctxcXF5Xjc8PBwiYqK8loAAEBg8lnXUteuXWXv3r1e64YNG2bGwUycOFGqV68uoaGhkpSUZKZdq+TkZDl69Ki0a9fOR6UGAAD+xGdBpnz58tKkSROvdWXLljXXjHGtHz58uIwfP15iYmJMy8qYMWNMiGnbtq2PSg0AAPyJTwf75mXevHkSHBxsWmR0NlJ8fLwsWrTI18UCAAB+wq+CzIYNG7xu6yDghQsXmgUAAMDvriMDAABQWAQZAABgLYIMAACwFkEGAABYiyADAACsRZABAADWIsgAAABrEWQAAIC1CDIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKxFkAEAANYiyAAAgNIVZLp06SJpaWnXrD9z5ozZBgAA4LdBZsOGDXLp0qVr1l+8eFH+9a9/FUW5AAAA8hQiBfDVV1+5f//mm28kJSXFffvq1auyZs0aueGGGwpySAAAgJIJMs2bN5egoCCzZNeFFBkZKc8++2zhSwMAAFBcQebw4cPiOI7UqVNHtm/fLpUrV3ZvCwsLkypVqkiZMmUKckgAAICSCTI1a9Y0PzMzMwv/iAAAAL4IMp72798v69evl5MnT14TbKZOnVoUZQMAACj6IPPSSy/JqFGjpFKlShIXF2fGzLjo7wQZAADgt0Fm5syZ8sQTT8jEiROLvkQAAADFeR2Z06dPS//+/QtzVwAAAN8GGQ0xH3/8cdGVAgAAoKS6lurVqydTpkyRrVu3StOmTSU0NNRr+8MPP1yYwwIAABRIkKMXhimg2rVr53zAoCA5dOiQ+Av9/qfo6GhJT0+XqKgoXxen1Kk1abWvi+DXjszqGZB1mJ/zAoCi+PwuVIuMXhgPAADAyjEyAAAA/qBQLTIPPvhgrtuXLFlS2PIAAAAUb5DR6deeLl++LPv27ZO0tLRsv0wSAADAb4LMypUrr1mnX1OgV/utW7duUZQLKBVsHMgLAAE5RiY4OFjGjx8v8+bNK6pDAgAAlNxg34MHD8qVK1eK8pAAAABF27WkLS+e9FI0J06ckNWrV8uQIUMKc0gAAICSCTJ79uy5plupcuXK8swzz+Q5owkAAMCnQWb9+vVFVgAAAIASDTIuP//8syQnJ5vfGzRoYFplAAAA/Hqw7/nz500XUtWqVaVjx45mqVatmgwfPlwuXLhQ9KUEAAAoqiCjg303btwoH3zwgbkIni7vvfeeWfe3v/2tMIcEAAAoma6ld955R95++23p1KmTe93dd98tkZGRMmDAAHn++ecLc1gAAIDib5HR7qPY2Nhr1lepUoWuJQAA4N9Bpl27djJt2jS5ePGie91vv/0m06dPN9sAAAD8tmtp/vz5ctddd8mNN94ozZo1M+u+/PJLCQ8Pl48//rioywgAAFB0LTJNmzaV/fv3S2JiojRv3twss2bNkgMHDkjjxo3zfRwdS3PLLbdIVFSUWbQ156OPPnJv1xafhIQEqVixopQrV0769esnqamphSkyAAAIQIVqkdEAo2NkRowY4bV+yZIl5toyEydOzNdxtEVHA1D9+vXN1xy88sor0rt3b3PlYA1E48aNM197sGLFComOjpbRo0dL3759ZfPmzYUpNgAACDBBjiaIAqpVq5a8/vrr0r59e6/127Ztk4EDB8rhw4cLXaCYmBh5+umn5d577zUX2NPH0d/Vd999J40aNZItW7ZI27Zt83W8M2fOmBCUnp5uWn1QsmpNWu3rIsAHjszq6esiALBcfj+/C9W1lJKSYi6Gl5UGD/3yyMK4evWqLF++3FxsT7uYdu3aJZcvX5Zu3bq592nYsKHUqFHDBJmcZGRkmJP3XAAAQGAqVJCpXr16tt07uk6v8FsQe/fuNeNfdKDwQw89JCtXrpSbb77ZhKWwsDCpUKGC1/7apaXbcuv20gTnWrSsAAAgMBVqjIyOjRk7dqxpMenSpYtZl5SUJBMmTCjwlX31O5q++OIL03SkF9kbMmSIuUJwYU2ePNlcedhFW2QIMwAABKZCBZlHH31UTp06Jf/xH/8hly5dMusiIiLMIF8NEgWhrS716tUzv7dq1Up27Ngh//3f/y333XefObZ+/YFnq4zOWoqLi8vxeNqyowsAAAh8hepaCgoKktmzZ5sZSlu3bjXXkPn1119l6tSpv7tAmZmZZpyLhprQ0FDT0uOi37R99OhRLroHAAAK3yLjomNbWrduXej7a+tNjx49zADes2fPmhlKGzZskLVr15rxLfpt2tpNpDOZdMTymDFjTIjJ74wlAAAQ2H5XkPm9Tp48Kf/+7/9uZjppcNGL42mIufPOO832efPmSXBwsLkQnrbSxMfHy6JFi3xZZAAAYPt1ZGzCdWR8i+vIICdcawaAz64jAwAA4A8IMgAAwFoEGQAAYC2CDAAAsBZBBgAAWIsgAwAArEWQAQAA1iLIAAAAaxFkAACAtQgyAADAWgQZAABgLYIMAACwFkEGAABYiyADAACsRZABAADWCvF1AVC0ak1anec+R2b1LLHjAMWN1ypQutEiAwAArEWQAQAA1iLIAAAAaxFkAACAtQgyAADAWgQZAABgLYIMAACwFkEGAABYiyADAACsRZABAADWIsgAAABrEWQAAIC1CDIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKxFkAEAANYiyAAAAGsRZAAAgLUIMgAAwFoEGQAAYK0QXxcA9qo1abWviwAAKOVokQEAANYiyAAAAGsRZAAAgLUIMgAAwFoEGQAAYC2CDAAAsJZPg0xiYqK0bt1aypcvL1WqVJE+ffpIcnKy1z4XL16UhIQEqVixopQrV0769esnqampPiszAADwHz4NMhs3bjQhZevWrbJu3Tq5fPmydO/eXc6fP+/eZ9y4cfLBBx/IihUrzP7Hjx+Xvn37+rLYAADAT/j0gnhr1qzxur1s2TLTMrNr1y7p2LGjpKeny8svvyyvv/66dOnSxeyzdOlSadSokQk/bdu29VHJAQCAP/CrMTIaXFRMTIz5qYFGW2m6devm3qdhw4ZSo0YN2bJlS7bHyMjIkDNnzngtAAAgMPlNkMnMzJSxY8fK7bffLk2aNDHrUlJSJCwsTCpUqOC1b2xsrNmW07ib6Oho91K9evUSKT8AACjFQUbHyuzbt0+WL1/+u44zefJk07LjWo4dO1ZkZQQAAP7FL740cvTo0fLhhx/Kpk2b5MYbb3Svj4uLk0uXLklaWppXq4zOWtJt2QkPDzcLAAAIfD5tkXEcx4SYlStXyqeffiq1a9f22t6qVSsJDQ2VpKQk9zqdnn306FFp166dD0oMAAD8SYivu5N0RtJ7771nriXjGveiY1siIyPNz+HDh8v48ePNAOCoqCgZM2aMCTHMWAIAAD4NMs8//7z52alTJ6/1OsV66NCh5vd58+ZJcHCwuRCezkiKj4+XRYsW+aS8AADAv4T4umspLxEREbJw4UKzAAAA+OWsJQAAgIIiyAAAAGsRZAAAgLUIMgAAwFp+cUE8AKVPrUmrfV0EAAGAFhkAAGAtggwAALAWQQYAAFiLIAMAAKxFkAEAANZi1lIpxGwRAECgoEUGAABYiyADAACsRZABAADWIsgAAABrEWQAAIC1CDIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKzFVxRYhK8WAPz//+CRWT1LpCwA/h9aZAAAgLUIMgAAwFoEGQAAYC2CDAAAsBZBBgAAWItZSwACHrONgMBFiwwAALAWQQYAAFiLIAMAAKxFkAEAANYiyAAAAGsRZAAAgLUIMgAAwFoEGQAAYC2CDAAAsBZBBgAAWIuvKACAEsZXJgBFhxYZAABgLYIMAACwFkEGAABYiyADAACsxWBfAMjnAFwA/ocWGQAAYC2fBplNmzZJr169pFq1ahIUFCSrVq3y2u44jkydOlWqVq0qkZGR0q1bN9m/f7/PygsAAPyLT4PM+fPnpVmzZrJw4cJstz/11FOyYMECWbx4sWzbtk3Kli0r8fHxcvHixRIvKwAA8D8+HSPTo0cPs2RHW2Pmz58vjz32mPTu3duse/XVVyU2Nta03AwcOLCESwsAAPyN346ROXz4sKSkpJjuJJfo6Ghp06aNbNmyJcf7ZWRkyJkzZ7wWAAAQmPx21pKGGKUtMJ70tmtbdhITE2X69OnFXj4AyA6zn4CS5bctMoU1efJkSU9Pdy/Hjh3zdZEAAEBpCzJxcXHmZ2pqqtd6ve3alp3w8HCJioryWgAAQGDy2yBTu3ZtE1iSkpLc63S8i85eateunU/LBgAA/INPx8icO3dODhw44DXA94svvpCYmBipUaOGjB07VmbOnCn169c3wWbKlCnmmjN9+vTxZbEBAICf8GmQ2blzp3Tu3Nl9e/z48ebnkCFDZNmyZTJhwgRzrZmRI0dKWlqadOjQQdasWSMRERE+LDUAlL4Bykdm9SyRsgBWBZlOnTqZ68XkRK/2O2PGDLMAAABYM0YGAAAgLwQZAABgLYIMAACwFkEGAABYy2+/osAGjPQHAMC3aJEBAADWIsgAAABrEWQAAIC1CDIAAMBaBBkAAGAtZi1ZMvsJAABcixYZAABgLYIMAACwFkEGAABYiyADAACsxWBfALAUX5MC0CIDAAAsRpABAADWIsgAAABrEWQAAIC1CDIAAMBazFoCAD/kb19dwgwp+CtaZAAAgLUIMgAAwFoEGQAAYC2CDAAAsBZBBgAAWIsgAwAArEWQAQAA1iLIAAAAaxFkAACAtQgyAADAWnxFQSm7zDgAlBZ8rULpQIsMAACwFkEGAABYiyADAACsRZABAADWIsgAAABrMWsJAAJYSc6cLKrHYiYRCoIWGQAAYC2CDAAAsBZBBgAAWIsgAwAArMVgXwCAX/G3AcpFNfi4JAdD1yqi87Lhax6saJFZuHCh1KpVSyIiIqRNmzayfft2XxcJAAD4Ab8PMm+++aaMHz9epk2bJrt375ZmzZpJfHy8nDx50tdFAwAAPub3QWbu3LkyYsQIGTZsmNx8882yePFiue6662TJkiW+LhoAAPAxvw4yly5dkl27dkm3bt3c64KDg83tLVu2+LRsAADA9/x6sO8vv/wiV69eldjYWK/1evu7777L9j4ZGRlmcUlPTzc/z5w5U+Tly8y4UOTHBAD4l6L6/Ciqz4z8lCczH49VkscpDNdxHcexN8gURmJiokyfPv2a9dWrV/dJeQAAdoueLwFZnmg/O05Ozp49K9HR0XYGmUqVKkmZMmUkNTXVa73ejouLy/Y+kydPNoODXTIzM+XXX3+VihUrSlBQUJGlRA1Gx44dk6ioqCI5ZqChjvKHesobdZQ36ih/qCe76khbYjTEVKtWLdf9/DrIhIWFSatWrSQpKUn69OnjDiZ6e/To0dneJzw83CyeKlSoUCzl0yfZ10+0v6OO8od6yht1lDfqKH+oJ3vqKLeWGCuCjNLWlSFDhsitt94qt912m8yfP1/Onz9vZjEBAIDSze+DzH333Sc///yzTJ06VVJSUqR58+ayZs2aawYAAwCA0sfvg4zSbqScupJ8Qbuu9AJ9Wbuw8P9RR/lDPeWNOsobdZQ/1FNg1lGQk9e8JgAAAD/l1xfEAwAAyA1BBgAAWIsgAwAArEWQAQAA1iLI5NMTTzwh7du3N9+8nd8L7Ok4ap02XrVqVYmMjDRfdrl//34JVHoF5cGDB5uLKGkdDR8+XM6dO5frfTp16mSuuOy5PPTQQxJIFi5cKLVq1ZKIiAhp06aNbN++Pdf9V6xYIQ0bNjT7N23aVP73f/9XAl1B6mjZsmXXvGb0foFs06ZN0qtXL3OFUz3fVatW5XmfDRs2SMuWLc3sk3r16pl6C2QFrSOtn6yvI130Mh+BKjExUVq3bi3ly5eXKlWqmAvNJicn53k/f39PIsgU4Ju4+/fvL6NGjcr3fZ566ilZsGCBLF68WLZt2yZly5aV+Ph4uXjxogQiDTFff/21rFu3Tj788EPzxjJy5Mg87zdixAg5ceKEe9F6CxRvvvmmuaijTmfcvXu3NGvWzLwGTp48me3+n3/+uQwaNMiEwD179pg3Gl327dsngaqgdaQ0LHu+Zn744QcJZHoRUK0XDXz5cfjwYenZs6d07txZvvjiCxk7dqz8+c9/lrVr10qgKmgduegHuedrST/gA9XGjRslISFBtm7dat6nL1++LN27dzd1lxMr3pN0+jXyb+nSpU50dHSe+2VmZjpxcXHO008/7V6XlpbmhIeHO2+88YYTaL755hudxu/s2LHDve6jjz5ygoKCnJ9++inH+91xxx3OX//6VydQ3XbbbU5CQoL79tWrV51q1ao5iYmJ2e4/YMAAp2fPnl7r2rRp4/zlL39xAlVB6yi//wcDlf4/W7lyZa77TJgwwWncuLHXuvvuu8+Jj493SoP81NH69evNfqdPn3ZKq5MnT5o62LhxY4772PCeRItMMdG/iLSJUruTPL8zQpvNt2zZIoFGz0m7k/SrJFz03IODg01rVG5ee+018wWhTZo0MV/6eeFC0XzVvT+04u3atcvrNaD1obdzeg3oes/9lbZOBOJrprB1pLTLsmbNmubL7Xr37m1aAlF6X0e/h14tXrv/77zzTtm8ebOUJunp6eZnTEyM1a8lK67sayNXP2vWr1LQ24HYB6vnlLVJNiQkxPwHye1877//fvOBpP3aX331lUycONE09b777rtiu19++UWuXr2a7Wvgu+++y/Y+Wlel5TVT2Dpq0KCBLFmyRG655RbzRjxnzhwzfk3DzI033lhCJfdvOb2O9JuNf/vtNzNmr7TT8KLd/vrHV0ZGhvzP//yPGbOnf3jp2KJAl5mZabocb7/9dvNHZE5seE8q1UFm0qRJMnv27Fz3+fbbb80gp9Iqv3VUWJ5jaHQQmb65dO3aVQ4ePCh169Yt9HERuNq1a2cWFw0xjRo1khdeeEEef/xxn5YN9tBArIvn60jfd+bNmyf/+Mc/JNAlJCSYcS6fffaZ2K5UB5m//e1vMnTo0Fz3qVOnTqGOHRcXZ36mpqaaD2cXva1NmYFWR3q+WQdnXrlyxcxkctVFfmjXmzpw4ID1QUa7y8qUKWOec096O6c60fUF2d92hamjrEJDQ6VFixbmNYPcX0c6SJrWmJzddtttAfHBnhf97kLXhIy8WjFteE8q1WNkKleubFpbclvCwsIKdezatWubJzopKcm9Tpt1tdnS86/JQKkjPae0tDQz3sHl008/Nc2XrnCSHzrDQnmGP1tpvbRq1crrNaD1obdzeg3oes/9lc4usOk1U9x1lJV2Te3duzcgXjNFpbS9joqKvv8E8uvIcRwTYlauXGnen/VzKiBeS74ebWyLH374wdmzZ48zffp0p1y5cuZ3Xc6ePevep0GDBs67777rvj1r1iynQoUKznvvved89dVXTu/evZ3atWs7v/32mxOI7rrrLqdFixbOtm3bnM8++8ypX7++M2jQIPf2H3/80dSRblcHDhxwZsyY4ezcudM5fPiwqac6deo4HTt2dALF8uXLzUy1ZcuWmZldI0eONK+JlJQUs/2BBx5wJk2a5N5/8+bNTkhIiDNnzhzn22+/daZNm+aEhoY6e/fudQJVQetI/w+uXbvWOXjwoLNr1y5n4MCBTkREhPP11187gUrfZ1zvOfq2PXfuXPO7vi8prR+tJ5dDhw451113nfPoo4+a19HChQudMmXKOGvWrHECVUHraN68ec6qVauc/fv3m/9fOnsyODjY+eSTT5xANWrUKDPjb8OGDc6JEyfcy4ULF9z72PieRJDJpyFDhpj/HFkXncLnord1aqjnFOwpU6Y4sbGx5o26a9euTnJyshOoTp06ZYKLBr2oqChn2LBhXkFPw4pnnR09etSElpiYGFM/9erVM2+86enpTiB59tlnnRo1ajhhYWFmqvHWrVu9pp/ra8vTW2+95dx0001mf51Cu3r1aifQFaSOxo4d695X/2/dfffdzu7du51A5poqnHVx1Yv+1HrKep/mzZubetI/EDzfmwJRQeto9uzZTt26dU0I1vegTp06OZ9++qkTyCSb+sn6uWXje1KQ/uPrViEAAIDCKNVjZAAAgN0IMgAAwFoEGQAAYC2CDAAAsBZBBgAAWIsgAwAArEWQAQAA1iLIALBSrVq1ZP78+b4uBgAfI8gAAABrEWQAAIC1CDIAStyLL74o1apVM9907al3797y4IMPysGDB83vsbGxUq5cOWndurV88sknOR7vyJEjEhQU5P72dKXfxq7rNmzY4F63b98+6dGjhzmmHvuBBx6QX375pZjOEkBJIMgAKHH9+/eXU6dOyfr1693rfv31V1mzZo0MHjxYzp07J3fffbckJSXJnj175K677pJevXrJ0aNHC/2YGmy6dOkiLVq0kJ07d5rHSk1NlQEDBhTRWQHwhRCfPCqAUu366683LSOvv/66dO3a1ax7++23pVKlStK5c2cJDg6WZs2aufd//PHHZeXKlfL+++/L6NGjC/WYzz33nAkxTz75pHvdkiVLpHr16vL999/LTTfdVARnBqCk0SIDwCe05eWdd96RjIwMc/u1116TgQMHmhCjLTKPPPKINGrUSCpUqGC6gr799tvf1SLz5ZdfmhYgPZZradiwodmmXVkA7ESLDACf0K4ix3Fk9erVZgzMv/71L5k3b57ZpiFm3bp1MmfOHKlXr55ERkbKvffeK5cuXcr2WBp+lB7P5fLly177aDjSx5w9e/Y1969atWoRnx2AkkKQAeATERER0rdvX9MSc+DAAWnQoIG0bNnSbNu8ebMMHTpU7rnnHncI0QG9OalcubL5eeLECdN9pDwH/io9trYA6fVnQkJ46wMCBV1LAHzavaQtMjpWRX93qV+/vrz77rsmjGiX0P3333/NDCdP2mLTtm1bmTVrlumC2rhxozz22GNe+yQkJJgBxYMGDZIdO3aY7qS1a9fKsGHD5OrVq8V6ngCKD0EGgM/oLKKYmBhJTk42YcVl7ty5ZkBw+/btTXdQfHy8u7UmJxqGrly5Iq1atZKxY8fKzJkzvbbrdG9t6dHQ0r17d2natKnZT8fguLqmANgnyPHsVAYAALAIf4YAAABrEWQAAIC1CDIAAMBaBBkAAGAtggwAALAWQQYAAFiLIAMAAKxFkAEAANYiyAAAAGsRZAAAgLUIMgAAwFoEGQAAILb6P3jWOGUCBxkyAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in mapping: 160\n",
      "Missing files: 0\n",
      "Bad shapes: 160  expected (1280,)\n",
      "NaN or Inf vectors: 0\n",
      "Example bad shape: [('Features/MNASNet/HandGesture/CurtainGesture/Hamad/CGH1.npy', (1000,)), ('Features/MNASNet/HandGesture/CurtainGesture/Hamad/CGH2.npy', (1000,)), ('Features/MNASNet/HandGesture/CurtainGesture/Hamad/CGH3.npy', (1000,))]\n",
      "Image: Processed Dataset/ProcessedHandGesture/LightGesture/Obaid/LGO6.png\n",
      "Label: light\n",
      "User: obaid\n",
      "Feature shape: (1000,)\n",
      "First 20 values: [-0.0158 -0.5674  0.1122  0.0677  0.2852 -0.1349 -0.1482 -0.3948 -0.3091\n",
      " -0.686  -0.179   0.0013 -0.384  -0.6147 -0.3943 -0.1243 -0.4824 -0.293\n",
      " -0.0631 -0.7959]\n",
      "Stats min max mean std: -1.17578125 2.24609375 0.00011118697875645012 0.4639372229576111\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the mapping CSV and feature files for issues, and print a random example\n",
    "def check_features(map_csv):\n",
    "    df = pd.read_csv(map_csv)\n",
    "    print(f\"Rows in mapping: {len(df)}\")\n",
    "\n",
    "    missing = []\n",
    "    bad_shape = []\n",
    "    nan_inf = 0\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        fp = (ROOT / row[\"feature_path\"])\n",
    "        if not fp.exists():\n",
    "            missing.append(row[\"feature_path\"])\n",
    "            continue\n",
    "        v = np.load(fp)\n",
    "        if v.shape != (FEATURE_DIM,):\n",
    "            bad_shape.append((row[\"feature_path\"], v.shape))\n",
    "        if not np.isfinite(v).all():\n",
    "            nan_inf += 1\n",
    "\n",
    "    print(f\"Missing files: {len(missing)}\")\n",
    "    print(f\"Bad shapes: {len(bad_shape)}  expected {(FEATURE_DIM,)}\")\n",
    "    print(f\"NaN or Inf vectors: {nan_inf}\")\n",
    "\n",
    "    if missing:\n",
    "        print(\"Example missing:\", missing[:3])\n",
    "    if bad_shape:\n",
    "        print(\"Example bad shape:\", bad_shape[:3])\n",
    "\n",
    "\n",
    "\n",
    "    row = df.sample(1, random_state=SEED).iloc[0]  # random pick\n",
    "    fp = ROOT / row[\"feature_path\"]\n",
    "    v = np.load(fp)\n",
    "\n",
    "    print(\"Image:\", row[\"image_path\"])\n",
    "    print(\"Label:\", row[\"label\"])\n",
    "    if \"user_id\" in row:\n",
    "        print(\"User:\", row[\"user_id\"])\n",
    "    print(\"Feature shape:\", v.shape)\n",
    "    print(\"First 20 values:\", np.round(v[:20], 4))\n",
    "    print(\"Stats min max mean std:\", float(v.min()), float(v.max()), float(v.mean()), float(v.std()))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(v, bins=50)\n",
    "    plt.title(\"Feature value distribution\")\n",
    "    plt.xlabel(\"value\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.show()\n",
    "\n",
    "# Feature example check\n",
    "check_features(MAP_CSV_FALL)\n",
    "check_features(MAP_CSV_GEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "c5bc6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Variables Declaration ----------\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "HIDDEN_DIM = 64\n",
    "DROPOUT = 0.3\n",
    "APPLY_STANDARDIZE = True     # fit on train only, apply to val/test\n",
    "APPLY_L2 = False             # optional per-sample L2 normalize\n",
    "MODELS_ROOT = ROOT / \"Models\"  # save trained models here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "c364a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Training and evaluation of MLP classifier on extracted features ----------\n",
    "\n",
    "# ---- Repro utilities (use existing SEED; no reinit of device) ----\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ---- Data loading from mapping CSV ----\n",
    "def load_features_from_map(map_csv: Path, has_user: bool):\n",
    "    df = pd.read_csv(map_csv)\n",
    "    X = np.stack([np.load((ROOT / fp).as_posix()) for fp in df[\"feature_path\"]]).astype(np.float32)\n",
    "    y_text = df[\"label\"].astype(str).values\n",
    "    enc = LabelEncoder()\n",
    "    y = enc.fit_transform(y_text)\n",
    "    meta = {\"labels_text\": y_text, \"label_encoder\": enc}\n",
    "    if has_user and \"user_id\" in df.columns:\n",
    "        meta[\"user_id\"] = df[\"user_id\"].astype(str).values\n",
    "    meta[\"image_path\"] = df[\"image_path\"].astype(str).values\n",
    "    return X, y, meta\n",
    "\n",
    "# Dataset class for loading numpy features\n",
    "class NumpyFeatDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y).long()\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
    "\n",
    "# Simple MLP classifier\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, HIDDEN_DIM),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(HIDDEN_DIM, num_classes),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "# Apply scaling (standardization + L2 normalization)\n",
    "def apply_scaling(X_train, X_val, X_test):\n",
    "    scaler = None\n",
    "    if APPLY_STANDARDIZE:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    if APPLY_L2:\n",
    "        X_train = X_train / (np.linalg.norm(X_train, axis=1, keepdims=True) + 1e-8)\n",
    "        X_val   = X_val   / (np.linalg.norm(X_val,   axis=1, keepdims=True) + 1e-8)\n",
    "        X_test  = X_test  / (np.linalg.norm(X_test,  axis=1, keepdims=True) + 1e-8)\n",
    "    return X_train.astype(np.float32), X_val.astype(np.float32), X_test.astype(np.float32), scaler\n",
    "\n",
    "# Train one split (train/val) and return best metrics + state\n",
    "def train_one_split(X_train, y_train, X_val, y_val, num_classes, seed):\n",
    "    # reuse earlier helpers/vars: seed_everything, apply_scaling, FEATURE_DIM, device,\n",
    "    # TRAIN_BATCH_SIZE, EPOCHS, LR, WEIGHT_DECAY, DROPOUT, HIDDEN_DIM\n",
    "    seed_everything(seed)\n",
    "\n",
    "    X_train_s, X_val_s, X_val_for_eval, scaler = apply_scaling(X_train, X_val, X_val.copy())\n",
    "\n",
    "    train_ds = NumpyFeatDataset(X_train_s, y_train)\n",
    "    val_ds   = NumpyFeatDataset(X_val_s,   y_val)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=0\n",
    "    )\n",
    "    val_loader   = torch.utils.data.DataLoader(\n",
    "        val_ds,   batch_size=TRAIN_BATCH_SIZE, shuffle=False, num_workers=0\n",
    "    )\n",
    "\n",
    "    model = MLP(FEATURE_DIM, num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    best = {\"acc\": 0.0, \"state\": None}\n",
    "\n",
    "    for _ in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "\n",
    "        # quick val\n",
    "        model.eval()\n",
    "        all_logits, all_y = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                all_logits.append(model(xb).cpu())\n",
    "                all_y.append(yb)\n",
    "        logits = torch.cat(all_logits); y_true = torch.cat(all_y)\n",
    "        y_pred = logits.argmax(1)\n",
    "        acc = (y_pred == y_true).float().mean().item()\n",
    "        if acc > best[\"acc\"]:\n",
    "            best[\"acc\"] = acc\n",
    "            best[\"state\"] = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "    # final metrics with best\n",
    "    model.load_state_dict(best[\"state\"])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.from_numpy(X_val_s).to(device)).cpu().numpy()\n",
    "    y_pred = logits.argmax(1)\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1m = f1_score(y_val, y_pred, average=\"macro\")\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1_macro\": f1m,\n",
    "        \"cm\": cm,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"state\": best[\"state\"],\n",
    "        \"scaler\": scaler,\n",
    "    }\n",
    "\n",
    "# Save all artifacts from one run\n",
    "def save_run_artifacts(save_dir: Path,\n",
    "                       state_dict: dict,\n",
    "                       scaler,\n",
    "                       label_encoder,\n",
    "                       config: dict,\n",
    "                       metrics: dict,\n",
    "                       cm: np.ndarray,\n",
    "                       class_names: list):\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # model\n",
    "    torch.save(state_dict, save_dir / \"model_state.pt\")\n",
    "\n",
    "    # scaler + label encoder\n",
    "    with open(save_dir / \"scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    with open(save_dir / \"label_encoder.pkl\", \"wb\") as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "\n",
    "    # config + metrics\n",
    "    with open(save_dir / \"config.json\", \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    with open(save_dir / \"metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    # confusion matrix (csv + png)\n",
    "    np.savetxt(save_dir / \"confusion_matrix.csv\", cm, fmt=\"%d\", delimiter=\",\")\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    ax.set_xticks(range(len(class_names))); ax.set_xticklabels(class_names, rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(range(len(class_names))); ax.set_yticklabels(class_names)\n",
    "    # annotate\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, int(cm[i, j]), ha=\"center\", va=\"center\", fontsize=8)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_dir / \"confusion_matrix.png\", dpi=160)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "6088e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Variables Declaration for EffB7 ----------\n",
    "MAP_CSV_FALL = ROOT / \"FallDetectionFeaturesEffB7.csv\"\n",
    "MAP_CSV_GEST = ROOT / \"HandGestureFeaturesEffB7.csv\"\n",
    "FEATURES_ROOT = ROOT / \"Features\" / \"EffB7\"\n",
    "FEATURE_DIM = 2560\n",
    "MODELS_ROOT = ROOT / \"ModelsB7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "a6710f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fall] Run 1: acc=0.850  f1_macro=0.847\n",
      "[Fall] Run 2: acc=0.950  f1_macro=0.947\n",
      "[Fall] Run 3: acc=0.900  f1_macro=0.896\n",
      "[Fall] Run 4: acc=1.000  f1_macro=1.000\n",
      "[Fall] Run 5: acc=0.950  f1_macro=0.947\n",
      "Fall avg acc: 0.93\n",
      "Fall avg f1 : 0.927\n"
     ]
    }
   ],
   "source": [
    "# Train and Test Fall 80/20 across 5 seeds\n",
    "N_SEEDS = 5\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "X_fall, y_fall, meta_fall = load_features_from_map(MAP_CSV_FALL, has_user=False)\n",
    "class_names_fall = meta_fall[\"label_encoder\"].classes_.tolist()\n",
    "num_classes_fall = len(class_names_fall)\n",
    "\n",
    "results_fall = []\n",
    "for i, seed in enumerate(range(SEED, SEED + N_SEEDS), 1):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=seed)\n",
    "    train_idx, val_idx = next(sss.split(X_fall, y_fall))\n",
    "\n",
    "    res = train_one_split(\n",
    "        X_fall[train_idx], y_fall[train_idx],\n",
    "        X_fall[val_idx],   y_fall[val_idx],\n",
    "        num_classes_fall, seed\n",
    "    )\n",
    "    results_fall.append(res)\n",
    "    print(f\"[Fall] Run {i}: acc={res['acc']:.3f}  f1_macro={res['f1_macro']:.3f}\")\n",
    "\n",
    "    # save\n",
    "    save_dir = MODELS_ROOT / \"Fall\" / f\"seed_{seed}\"\n",
    "    cfg = {\n",
    "        \"dataset\": \"Fall\",\n",
    "        \"seed\": seed,\n",
    "        \"feature_dim\": FEATURE_DIM,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"lr\": LR,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"apply_standardize\": APPLY_STANDARDIZE,\n",
    "        \"apply_l2\": APPLY_L2,\n",
    "        \"classes\": class_names_fall,\n",
    "    }\n",
    "    metrics = {\"acc\": float(res[\"acc\"]), \"f1_macro\": float(res[\"f1_macro\"])}\n",
    "    save_run_artifacts(save_dir, res[\"state\"], res[\"scaler\"],\n",
    "                       meta_fall[\"label_encoder\"], cfg, metrics, res[\"cm\"], class_names_fall)\n",
    "\n",
    "print(\"Fall avg acc:\", np.mean([r[\"acc\"] for r in results_fall]).round(3))\n",
    "print(\"Fall avg f1 :\", np.mean([r[\"f1_macro\"] for r in results_fall]).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "39f22037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gesture LOSO] Holdout hamad: acc=0.900  f1_macro=0.893\n",
      "[Gesture LOSO] Holdout mohammad: acc=0.950  f1_macro=0.950\n",
      "[Gesture LOSO] Holdout obaid: acc=0.950  f1_macro=0.950\n",
      "[Gesture LOSO] Holdout saif: acc=0.950  f1_macro=0.949\n",
      "Gesture LOSO avg acc: 0.938\n",
      "Gesture LOSO avg f1 : 0.936\n"
     ]
    }
   ],
   "source": [
    "# Train and Test Gesture LOSO (train on 3 users, test on 1)\n",
    "X_gest, y_gest, meta_gest = load_features_from_map(MAP_CSV_GEST, has_user=True)\n",
    "users = np.array(meta_gest[\"user_id\"])\n",
    "class_names_gest = meta_gest[\"label_encoder\"].classes_.tolist()\n",
    "num_classes_gest = len(class_names_gest)\n",
    "\n",
    "unique_users = sorted(np.unique(users))\n",
    "results_gest_loso = []\n",
    "\n",
    "for u in unique_users:\n",
    "    test_mask = (users == u)\n",
    "    train_mask = ~test_mask\n",
    "\n",
    "    res = train_one_split(\n",
    "        X_gest[train_mask], y_gest[train_mask],\n",
    "        X_gest[test_mask],  y_gest[test_mask],\n",
    "        num_classes_gest, seed=SEED\n",
    "    )\n",
    "    results_gest_loso.append((u, res))\n",
    "    print(f\"[Gesture LOSO] Holdout {u}: acc={res['acc']:.3f}  f1_macro={res['f1_macro']:.3f}\")\n",
    "\n",
    "    save_dir = MODELS_ROOT / \"Gesture_LOSO\" / f\"user_{u}\"\n",
    "    cfg = {\n",
    "        \"dataset\": \"Gesture_LOSO\",\n",
    "        \"holdout_user\": u,\n",
    "        \"seed\": SEED,\n",
    "        \"feature_dim\": FEATURE_DIM,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"lr\": LR,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"apply_standardize\": APPLY_STANDARDIZE,\n",
    "        \"apply_l2\": APPLY_L2,\n",
    "        \"classes\": class_names_gest,\n",
    "    }\n",
    "    metrics = {\"acc\": float(res[\"acc\"]), \"f1_macro\": float(res[\"f1_macro\"])}\n",
    "    save_run_artifacts(save_dir, res[\"state\"], res[\"scaler\"],\n",
    "                       meta_gest[\"label_encoder\"], cfg, metrics, res[\"cm\"], class_names_gest)\n",
    "\n",
    "print(\"Gesture LOSO avg acc:\", np.mean([r[1]['acc'] for r in results_gest_loso]).round(3))\n",
    "print(\"Gesture LOSO avg f1 :\", np.mean([r[1]['f1_macro'] for r in results_gest_loso]).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "c527e597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gesture Mixed] Run 1: acc=1.000  f1_macro=1.000\n",
      "[Gesture Mixed] Run 2: acc=1.000  f1_macro=1.000\n",
      "[Gesture Mixed] Run 3: acc=1.000  f1_macro=1.000\n",
      "[Gesture Mixed] Run 4: acc=1.000  f1_macro=1.000\n",
      "[Gesture Mixed] Run 5: acc=0.969  f1_macro=0.969\n",
      "Gesture Mixed avg acc: 0.994\n",
      "Gesture Mixed avg f1 : 0.994\n"
     ]
    }
   ],
   "source": [
    "# Train and Test Gesture 80/20 across 5 seeds (mixed users)\n",
    "N_SEEDS = 5\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "results_gest_mix = []\n",
    "for i, seed in enumerate(range(SEED, SEED + N_SEEDS), 1):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=seed)\n",
    "    train_idx, val_idx = next(sss.split(X_gest, y_gest))\n",
    "\n",
    "    res = train_one_split(\n",
    "        X_gest[train_idx], y_gest[train_idx],\n",
    "        X_gest[val_idx],   y_gest[val_idx],\n",
    "        num_classes_gest, seed\n",
    "    )\n",
    "    results_gest_mix.append(res)\n",
    "    print(f\"[Gesture Mixed] Run {i}: acc={res['acc']:.3f}  f1_macro={res['f1_macro']:.3f}\")\n",
    "\n",
    "    save_dir = MODELS_ROOT / \"Gesture_Mixed\" / f\"seed_{seed}\"\n",
    "    cfg = {\n",
    "        \"dataset\": \"Gesture_Mixed\",\n",
    "        \"seed\": seed,\n",
    "        \"feature_dim\": FEATURE_DIM,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"lr\": LR,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"apply_standardize\": APPLY_STANDARDIZE,\n",
    "        \"apply_l2\": APPLY_L2,\n",
    "        \"classes\": class_names_gest,\n",
    "    }\n",
    "    metrics = {\"acc\": float(res[\"acc\"]), \"f1_macro\": float(res[\"f1_macro\"])}\n",
    "    save_run_artifacts(save_dir, res[\"state\"], res[\"scaler\"],\n",
    "                       meta_gest[\"label_encoder\"], cfg, metrics, res[\"cm\"], class_names_gest)\n",
    "\n",
    "print(\"Gesture Mixed avg acc:\", np.mean([r[\"acc\"] for r in results_gest_mix]).round(3))\n",
    "print(\"Gesture Mixed avg f1 :\", np.mean([r[\"f1_macro\"] for r in results_gest_mix]).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "37c14fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Variables Declaration for Lite0 ----------\n",
    "MAP_CSV_FALL = ROOT / \"FallDetectionFeaturesEffLite0.csv\"\n",
    "MAP_CSV_GEST = ROOT / \"HandGestureFeaturesEffLite0.csv\"\n",
    "FEATURES_ROOT = ROOT / \"Features\" / \"EffLite0\"\n",
    "FEATURE_DIM = 1280\n",
    "MODELS_ROOT = ROOT / \"ModelsLite0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "1ed732aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fall] Run 1: acc=0.950  f1_macro=0.949\n",
      "[Fall] Run 2: acc=1.000  f1_macro=1.000\n",
      "[Fall] Run 3: acc=1.000  f1_macro=1.000\n",
      "[Fall] Run 4: acc=1.000  f1_macro=1.000\n",
      "[Fall] Run 5: acc=1.000  f1_macro=1.000\n",
      "Fall avg acc: 0.99\n",
      "Fall avg f1 : 0.99\n"
     ]
    }
   ],
   "source": [
    "# Train and Test Fall 80/20 across 5 seeds\n",
    "N_SEEDS = 5\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "X_fall, y_fall, meta_fall = load_features_from_map(MAP_CSV_FALL, has_user=False)\n",
    "class_names_fall = meta_fall[\"label_encoder\"].classes_.tolist()\n",
    "num_classes_fall = len(class_names_fall)\n",
    "\n",
    "results_fall = []\n",
    "for i, seed in enumerate(range(SEED, SEED + N_SEEDS), 1):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=seed)\n",
    "    train_idx, val_idx = next(sss.split(X_fall, y_fall))\n",
    "\n",
    "    res = train_one_split(\n",
    "        X_fall[train_idx], y_fall[train_idx],\n",
    "        X_fall[val_idx],   y_fall[val_idx],\n",
    "        num_classes_fall, seed\n",
    "    )\n",
    "    results_fall.append(res)\n",
    "    print(f\"[Fall] Run {i}: acc={res['acc']:.3f}  f1_macro={res['f1_macro']:.3f}\")\n",
    "\n",
    "    # save\n",
    "    save_dir = MODELS_ROOT / \"Fall\" / f\"seed_{seed}\"\n",
    "    cfg = {\n",
    "        \"dataset\": \"Fall\",\n",
    "        \"seed\": seed,\n",
    "        \"feature_dim\": FEATURE_DIM,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"lr\": LR,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"apply_standardize\": APPLY_STANDARDIZE,\n",
    "        \"apply_l2\": APPLY_L2,\n",
    "        \"classes\": class_names_fall,\n",
    "    }\n",
    "    metrics = {\"acc\": float(res[\"acc\"]), \"f1_macro\": float(res[\"f1_macro\"])}\n",
    "    save_run_artifacts(save_dir, res[\"state\"], res[\"scaler\"],\n",
    "                       meta_fall[\"label_encoder\"], cfg, metrics, res[\"cm\"], class_names_fall)\n",
    "\n",
    "print(\"Fall avg acc:\", np.mean([r[\"acc\"] for r in results_fall]).round(3))\n",
    "print(\"Fall avg f1 :\", np.mean([r[\"f1_macro\"] for r in results_fall]).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "03b77dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gesture LOSO] Holdout hamad: acc=0.975  f1_macro=0.975\n",
      "[Gesture LOSO] Holdout mohammad: acc=1.000  f1_macro=1.000\n",
      "[Gesture LOSO] Holdout obaid: acc=1.000  f1_macro=1.000\n",
      "[Gesture LOSO] Holdout saif: acc=0.800  f1_macro=0.790\n",
      "Gesture LOSO avg acc: 0.944\n",
      "Gesture LOSO avg f1 : 0.941\n"
     ]
    }
   ],
   "source": [
    "# Train and Test Gesture LOSO (train on 3 users, test on 1)\n",
    "X_gest, y_gest, meta_gest = load_features_from_map(MAP_CSV_GEST, has_user=True)\n",
    "users = np.array(meta_gest[\"user_id\"])\n",
    "class_names_gest = meta_gest[\"label_encoder\"].classes_.tolist()\n",
    "num_classes_gest = len(class_names_gest)\n",
    "\n",
    "unique_users = sorted(np.unique(users))\n",
    "results_gest_loso = []\n",
    "\n",
    "for u in unique_users:\n",
    "    test_mask = (users == u)\n",
    "    train_mask = ~test_mask\n",
    "\n",
    "    res = train_one_split(\n",
    "        X_gest[train_mask], y_gest[train_mask],\n",
    "        X_gest[test_mask],  y_gest[test_mask],\n",
    "        num_classes_gest, seed=SEED\n",
    "    )\n",
    "    results_gest_loso.append((u, res))\n",
    "    print(f\"[Gesture LOSO] Holdout {u}: acc={res['acc']:.3f}  f1_macro={res['f1_macro']:.3f}\")\n",
    "\n",
    "    save_dir = MODELS_ROOT / \"Gesture_LOSO\" / f\"user_{u}\"\n",
    "    cfg = {\n",
    "        \"dataset\": \"Gesture_LOSO\",\n",
    "        \"holdout_user\": u,\n",
    "        \"seed\": SEED,\n",
    "        \"feature_dim\": FEATURE_DIM,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"lr\": LR,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"apply_standardize\": APPLY_STANDARDIZE,\n",
    "        \"apply_l2\": APPLY_L2,\n",
    "        \"classes\": class_names_gest,\n",
    "    }\n",
    "    metrics = {\"acc\": float(res[\"acc\"]), \"f1_macro\": float(res[\"f1_macro\"])}\n",
    "    save_run_artifacts(save_dir, res[\"state\"], res[\"scaler\"],\n",
    "                       meta_gest[\"label_encoder\"], cfg, metrics, res[\"cm\"], class_names_gest)\n",
    "\n",
    "print(\"Gesture LOSO avg acc:\", np.mean([r[1]['acc'] for r in results_gest_loso]).round(3))\n",
    "print(\"Gesture LOSO avg f1 :\", np.mean([r[1]['f1_macro'] for r in results_gest_loso]).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "bee4d461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gesture Mixed] Run 1: acc=1.000  f1_macro=1.000\n",
      "[Gesture Mixed] Run 2: acc=1.000  f1_macro=1.000\n",
      "[Gesture Mixed] Run 3: acc=1.000  f1_macro=1.000\n",
      "[Gesture Mixed] Run 4: acc=0.969  f1_macro=0.969\n",
      "[Gesture Mixed] Run 5: acc=1.000  f1_macro=1.000\n",
      "Gesture Mixed avg acc: 0.994\n",
      "Gesture Mixed avg f1 : 0.994\n"
     ]
    }
   ],
   "source": [
    "# Train and Test Gesture 80/20 across 5 seeds (mixed users)\n",
    "N_SEEDS = 5\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "results_gest_mix = []\n",
    "for i, seed in enumerate(range(SEED, SEED + N_SEEDS), 1):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=seed)\n",
    "    train_idx, val_idx = next(sss.split(X_gest, y_gest))\n",
    "\n",
    "    res = train_one_split(\n",
    "        X_gest[train_idx], y_gest[train_idx],\n",
    "        X_gest[val_idx],   y_gest[val_idx],\n",
    "        num_classes_gest, seed\n",
    "    )\n",
    "    results_gest_mix.append(res)\n",
    "    print(f\"[Gesture Mixed] Run {i}: acc={res['acc']:.3f}  f1_macro={res['f1_macro']:.3f}\")\n",
    "\n",
    "    save_dir = MODELS_ROOT / \"Gesture_Mixed\" / f\"seed_{seed}\"\n",
    "    cfg = {\n",
    "        \"dataset\": \"Gesture_Mixed\",\n",
    "        \"seed\": seed,\n",
    "        \"feature_dim\": FEATURE_DIM,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"lr\": LR,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"apply_standardize\": APPLY_STANDARDIZE,\n",
    "        \"apply_l2\": APPLY_L2,\n",
    "        \"classes\": class_names_gest,\n",
    "    }\n",
    "    metrics = {\"acc\": float(res[\"acc\"]), \"f1_macro\": float(res[\"f1_macro\"])}\n",
    "    save_run_artifacts(save_dir, res[\"state\"], res[\"scaler\"],\n",
    "                       meta_gest[\"label_encoder\"], cfg, metrics, res[\"cm\"], class_names_gest)\n",
    "\n",
    "print(\"Gesture Mixed avg acc:\", np.mean([r[\"acc\"] for r in results_gest_mix]).round(3))\n",
    "print(\"Gesture Mixed avg f1 :\", np.mean([r[\"f1_macro\"] for r in results_gest_mix]).round(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
