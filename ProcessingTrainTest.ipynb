{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5752351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Install necessary packages ----------\n",
    "%pip install numpy pandas opencv-python scikit-learn matplotlib tqdm\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e9e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1 — Imports (project-wide)\n",
    "# =========================\n",
    "\n",
    "# Core + IO\n",
    "import os, json, pickle, math, random, gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import PIL.Image as Image\n",
    "\n",
    "# Torch + Torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bda1a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 5070 Ti\n",
      "CUDA is available! Training on GPU...\n"
     ]
    }
   ],
   "source": [
    "# CELL 2 — Device Config & Cuda Setup\n",
    "# =========================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA is available! Training on GPU...\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Training on CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d1b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3 — Variable Config / Globals (project-wide)\n",
    "# =========================\n",
    "\n",
    "# ----- PATHS -----\n",
    "ROOT = Path(\".\")\n",
    "UNIFIED_CSV = ROOT / \"DatasetLabel.csv\"\n",
    "MODELS_UNIFIED_ROOT = ROOT / \"Models\" / \"Unified\"\n",
    "\n",
    "# ----- DATA / MODEL CONFIG -----\n",
    "IMAGE_SIZE = 224\n",
    "SAMPLE_EVERY = 1\n",
    "NUM_CLASSES = 6\n",
    "CLASS_NAMES = [\"fall\", \"light\", \"fan\", \"curtain\", \"screen\", \"none\"]\n",
    "LABEL_TO_IDX = {c: i for i, c in enumerate(CLASS_NAMES)}\n",
    "OVERWRITE = True\n",
    "\n",
    "# --- thresholds (tune if needed) ---\n",
    "THRESH_PERCENTILE = 92.5   # on non-zero diffs\n",
    "THRESH_MIN        = 2.0    # absolute min diff\n",
    "VIDEO_EXTS = [\".mp4\", \".avi\", \".mov\", \".mkv\", \".MP4\", \".AVI\", \".MOV\", \".MKV\"]\n",
    "\n",
    "# ----- MODEL CHOICE -----\n",
    "MODEL_NAME = \"efficientnet_v2_s\"\n",
    "\n",
    "# ----- TRAINING HYPERPARAMS -----\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VAL_BATCH_SIZE   = 64\n",
    "EPOCHS           = 200\n",
    "LR               = 1e-3\n",
    "WEIGHT_DECAY     = 1e-5\n",
    "DROPOUT          = 0.3\n",
    "HEAD_HIDDEN      = 128\n",
    "FREEZE_BACKBONE  = True         # freeze conv backbone for stability on small data\n",
    "USE_AMP_TRAIN    = True         # mixed precision on GPU\n",
    "\n",
    "# ----- SPLIT SETTINGS -----\n",
    "LOSO_USERS = [\"hamad\", \"mohammad\", \"obaid\", \"saif\"]     # gesture user ids\n",
    "LOSO_TEST_SIZE_FALLNONE = 0.25                          # 75/25 split for fall/none per LOSO fold\n",
    "MIXED_SEEDS = 5\n",
    "MIXED_TEST_SIZE = 0.20                                  # 80/20 split for mixed data\n",
    "\n",
    "# ----- REPRO -----\n",
    "def seed_everything(seed: int):\n",
    "    \"\"\"Set seeds and deterministic flags so runs are repeatable.\"\"\"\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b701bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [01:13<00:00,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All videos processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CELL 4 — Video → Motion Image (writes to image_path from UNIFIED_CSV)\n",
    "# =========================\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def resolve_video_path(rel_path: str) -> Path:\n",
    "    p = ROOT / rel_path\n",
    "    if p.is_file():\n",
    "        return p\n",
    "    if p.suffix == \"\":\n",
    "        for ext in VIDEO_EXTS:\n",
    "            cand = p.with_suffix(ext)\n",
    "            if cand.is_file():\n",
    "                return cand\n",
    "    if p.is_dir():\n",
    "        for ext in VIDEO_EXTS:\n",
    "            vids = sorted(p.glob(f\"*{ext}\"))\n",
    "            if vids:\n",
    "                return vids[0]\n",
    "    raise FileNotFoundError(f\"Video not found for: {rel_path}\")\n",
    "\n",
    "def pad_to_square(img: np.ndarray) -> np.ndarray:\n",
    "    h, w = img.shape[:2]\n",
    "    s = max(h, w)\n",
    "    top = (s - h) // 2\n",
    "    bottom = s - h - top\n",
    "    left = (s - w) // 2\n",
    "    right = s - w - left\n",
    "    return cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=0)\n",
    "\n",
    "# ----- Process video to motion image -----\n",
    "def process_video_to_motion_image(video_path: Path,\n",
    "                                  sample_every: int,\n",
    "                                  out_size: int) -> np.ndarray:\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Failed to open {video_path}\")\n",
    "\n",
    "    sampled = []\n",
    "    idx = 0\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "        if idx % sample_every == 0:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
    "            sampled.append(gray)\n",
    "        idx += 1\n",
    "    cap.release()\n",
    "\n",
    "    if len(sampled) < 2:\n",
    "        raise RuntimeError(f\"Not enough sampled frames ({len(sampled)}) in {video_path}\")\n",
    "\n",
    "    acc = np.zeros_like(sampled[0], dtype=np.float32)\n",
    "    prev = sampled[0]\n",
    "    for cur in sampled[1:]:\n",
    "        diff = np.abs(cur - prev)\n",
    "        nz = diff[diff > 0]\n",
    "        if nz.size > 0:\n",
    "            thr = max(np.percentile(nz, THRESH_PERCENTILE), THRESH_MIN)\n",
    "        else:\n",
    "            thr = THRESH_MIN\n",
    "        mask = diff >= thr\n",
    "        acc[mask] += diff[mask]\n",
    "        prev = cur\n",
    "\n",
    "    m = acc.max()\n",
    "    if m > 0:\n",
    "        acc = acc / m\n",
    "    acc = (acc * 255.0).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    acc = pad_to_square(acc)\n",
    "    acc = cv2.resize(acc, (out_size, out_size), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # stack grayscale to 3 channels for the backbone\n",
    "    img3 = np.stack([acc, acc, acc], axis=2)\n",
    "    return img3\n",
    "\n",
    "# ---- run over UNIFIED_CSV ----\n",
    "df_u = pd.read_csv(UNIFIED_CSV)\n",
    "\n",
    "for row in tqdm(df_u.itertuples(index=False), total=len(df_u)):\n",
    "    rel_video = getattr(row, \"video_path\")\n",
    "    rel_image = getattr(row, \"image_path\")\n",
    "\n",
    "    try:\n",
    "        vid_abs = resolve_video_path(rel_video)\n",
    "        img_abs = (ROOT / rel_image)\n",
    "        ensure_dir(img_abs.parent)\n",
    "\n",
    "        if OVERWRITE or not img_abs.exists():\n",
    "            img = process_video_to_motion_image(\n",
    "                vid_abs, sample_every=SAMPLE_EVERY, out_size=IMAGE_SIZE\n",
    "            )\n",
    "            ok = cv2.imwrite(str(img_abs), cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "            if not ok:\n",
    "                raise RuntimeError(\"cv2.imwrite failed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed processing {rel_video}: {e}\")\n",
    "\n",
    "print(\"All videos processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a1901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5 — Transforms + Dataset Class\n",
    "# =========================\n",
    "\n",
    "# Basic normalization and slight training augmentations\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.45),               # Add RandomErasing to train transforms\n",
    "    transforms.RandomHorizontalFlip(p=0.3),         # Add RandomHorizontalFlip to train transforms\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "class UnifiedMotionDataset(Dataset):\n",
    "    \"\"\"Dataset that reads image paths and labels from a unified CSV using provided row indices.\"\"\"\n",
    "    def __init__(self, csv_path: Path, root: Path, \n",
    "                 indices: np.ndarray, tfms):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.root = root\n",
    "        self.indices = indices\n",
    "        self.tfms = tfms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[self.indices[i]]\n",
    "        img_abs = self.root / row[\"image_path\"]\n",
    "        bgr = cv2.imread(str(img_abs), cv2.IMREAD_COLOR)\n",
    "        if bgr is None:\n",
    "            raise FileNotFoundError(f\"Missing image: {img_abs}\")\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        pil = Image.fromarray(rgb)  \n",
    "        x = self.tfms(pil)\n",
    "        y = LABEL_TO_IDX[row[\"label\"]]\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa166deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5 — CNN Model, Backbone + Head\n",
    "# =========================\n",
    "\n",
    "def make_backbone_and_dim(model_name: str = \"efficientnet_v2_s\"):\n",
    "    \"\"\"Create conv backbone and return (module, feature_channels). \"\"\"\n",
    "    if model_name == \"efficientnet_v2_s\":\n",
    "        m = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.DEFAULT)\n",
    "    else:\n",
    "        raise ValueError(\"Only 'efficientnetv2_s' supported in this cell.\")\n",
    "\n",
    "    backbone = m.features  # conv feature extractor (no classifier)\n",
    "    # Infer channel dim with a dummy forward at current IMAGE_SIZE\n",
    "    with torch.no_grad():\n",
    "        backbone.eval()\n",
    "        dummy = torch.zeros(1, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "        feats = backbone(dummy)\n",
    "        feat_dim = feats.shape[1]  # channels\n",
    "    return backbone, feat_dim\n",
    "\n",
    "# ----- UnifiedNet Model -----\n",
    "class UnifiedNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple unified classifier:\n",
    "      - conv backbone (EfficientNetV2-S)\n",
    "      - global max pooling (keeps strong motion edges)\n",
    "      - Linear -> BN -> ReLU -> Dropout -> Linear (to NUM_CLASSES)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_classes: int,\n",
    "                 model_name: str = \"efficientnet_v2_s\",\n",
    "                 freeze_backbone: bool = True,\n",
    "                 head_hidden: int = 128,\n",
    "                 dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.backbone, feat_dim = make_backbone_and_dim(model_name)\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.bn   = nn.BatchNorm1d(head_hidden)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc1  = nn.Linear(feat_dim, head_hidden)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2  = nn.Linear(head_hidden, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)           # [B, C, H, W]\n",
    "        x = self.pool(x).flatten(1)    # [B, C]\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cce779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7 — Train/Eval + Saving helpers\n",
    "# =========================\n",
    "\n",
    "def save_unified_artifacts(save_dir: Path,\n",
    "                           state_dict: dict,\n",
    "                           config: dict,\n",
    "                           metrics: dict,\n",
    "                           cm: np.ndarray,\n",
    "                           class_names: list):\n",
    "    \"\"\"Write model weights, config, metrics, and confusion matrix (csv+png).\"\"\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(state_dict, save_dir / \"model_state.pt\")\n",
    "    with open(save_dir / \"config.json\", \"w\") as f: json.dump(config, f, indent=2)\n",
    "    with open(save_dir / \"metrics.json\", \"w\") as f: json.dump(metrics, f, indent=2)\n",
    "    np.savetxt(save_dir / \"confusion_matrix.csv\", cm, fmt=\"%d\", delimiter=\",\")\n",
    "\n",
    "    # Quick CM plot\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    ax.set_xticks(range(len(class_names))); ax.set_xticklabels(class_names, rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(range(len(class_names))); ax.set_yticklabels(class_names)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, int(cm[i, j]), ha=\"center\", va=\"center\", fontsize=8)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_dir / \"confusion_matrix.png\", dpi=160)\n",
    "    plt.close(fig)\n",
    "\n",
    "def train_eval_one_split(train_idx, test_idx, \n",
    "                         seed: int, tag: str):\n",
    "    \"\"\"\n",
    "    Train once on given train/test indices and return metrics + best state.\n",
    "    - Keeps the backbone frozen by default.\n",
    "    - Uses CrossEntropy + AdamW.\n",
    "    - Tracks best val f1 score over epochs and returns that checkpoint.\n",
    "    \"\"\"\n",
    "    seed_everything(seed)\n",
    "\n",
    "    # Data loaders\n",
    "    train_ds = UnifiedMotionDataset(UNIFIED_CSV, ROOT, train_idx, train_tfms)\n",
    "    test_ds  = UnifiedMotionDataset(UNIFIED_CSV, ROOT, test_idx,  val_tfms)\n",
    "    train_loader = DataLoader(train_ds, batch_size=TRAIN_BATCH_SIZE, shuffle=True,  num_workers=0)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=VAL_BATCH_SIZE,   shuffle=False, num_workers=0)\n",
    "\n",
    "    # Model, loss, optimizer\n",
    "    model = UnifiedNet(num_classes=NUM_CLASSES,\n",
    "                       model_name=MODEL_NAME,\n",
    "                       freeze_backbone=FREEZE_BACKBONE,\n",
    "                       head_hidden=HEAD_HIDDEN,\n",
    "                       dropout=DROPOUT).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                  lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # Track best-f1m checkpoint\n",
    "    best = {\"f1m\": 0.0, \"state\": None}\n",
    "\n",
    "    # Clear cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Train for EPOCHS, check test set each epoch (small data; OK for quick selection)\n",
    "    for _ in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            with torch.amp.autocast(\"cuda\", enabled=(USE_AMP_TRAIN and device.type == \"cuda\")):\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # quick eval\n",
    "        model.eval()\n",
    "        all_logits, all_y = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb = xb.to(device)\n",
    "                all_logits.append(model(xb).cpu())\n",
    "                all_y.append(yb)\n",
    "        logits = torch.cat(all_logits); y_true = torch.cat(all_y)\n",
    "        y_pred = logits.argmax(1)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        if f1m > best[\"f1m\"]:\n",
    "            best[\"f1m\"] = f1m\n",
    "            best[\"state\"] = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "    # Final metrics with best weights\n",
    "    model.load_state_dict(best[\"state\"])\n",
    "    model.eval()\n",
    "    y_true_all, y_pred_all, y_prob_all = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb).cpu()\n",
    "            probs  = torch.softmax(logits, dim=1)       # softmax = confidence per class\n",
    "            y_prob_all.append(probs)\n",
    "            y_pred_all.append(logits.argmax(1))\n",
    "            y_true_all.append(yb)\n",
    "    y_true = torch.cat(y_true_all).numpy()\n",
    "    y_pred = torch.cat(y_pred_all).numpy()\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(NUM_CLASSES)))\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    cfg = {\n",
    "        \"image_size\": IMAGE_SIZE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"lr\": LR,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"head_hidden\": HEAD_HIDDEN,\n",
    "        \"freeze_backbone\": FREEZE_BACKBONE,\n",
    "        \"backbone\": MODEL_NAME,\n",
    "        \"classes\": CLASS_NAMES,\n",
    "        \"seed\": seed,\n",
    "        \"tag\": tag,\n",
    "    }\n",
    "    # metrics = {\"acc\": float(acc), \"f1_macro\": float(f1m)}\n",
    "    return {\"acc\": acc, \"f1_macro\": f1m, \"cm\": cm, \"cfg\": cfg, \"state\": best[\"state\"], \"tag\": tag}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be34e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8 — Build split indices (LOSO and Mixed)\n",
    "# =========================\n",
    "\n",
    "# Load once to build masks\n",
    "df_unified = pd.read_csv(UNIFIED_CSV).reset_index(drop=True)\n",
    "labels = df_unified[\"label\"].astype(str).values\n",
    "usercol = df_unified.get(\"user_id\", pd.Series([\"\"]*len(df_unified))).astype(str).values\n",
    "\n",
    "# Masks for task parts\n",
    "is_gesture  = np.isin(labels, [\"light\",\"fan\",\"curtain\",\"screen\"])\n",
    "is_fallnone = np.isin(labels, [\"fall\",\"none\"])\n",
    "\n",
    "all_idx = np.arange(len(df_unified))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dd0245c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOSO hamad] acc=0.969  f1_macro=0.970\n",
      "[LOSO mohammad] acc=0.969  f1_macro=0.972\n",
      "[LOSO obaid] acc=0.908  f1_macro=0.902\n",
      "[LOSO saif] acc=0.800  f1_macro=0.792\n",
      "LOSO avg acc: 0.912\n",
      "LOSO avg f1 : 0.909\n"
     ]
    }
   ],
   "source": [
    "# CELL 9 — LOSO experiment\n",
    "# =========================\n",
    "results_loso = []\n",
    "\n",
    "for u in LOSO_USERS:\n",
    "    # Gesture test/train by user id\n",
    "    gest_test_idx  = all_idx[(is_gesture)  & (usercol == u)]\n",
    "    gest_train_idx = all_idx[(is_gesture)  & (usercol != u)]\n",
    "\n",
    "    # Fall+none split (unique per user via seed)\n",
    "    seed = LOSO_USERS.index(u) + 1  # seeds 1..4\n",
    "    seed_everything(seed)\n",
    "    fn_idx = all_idx[is_fallnone]\n",
    "    fn_labels = labels[is_fallnone]\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=LOSO_TEST_SIZE_FALLNONE, random_state=seed)\n",
    "    fn_train_sub, fn_test_sub = next(sss.split(fn_idx, fn_labels))\n",
    "    fn_train_idx = fn_idx[fn_train_sub]\n",
    "    fn_test_idx  = fn_idx[fn_test_sub]\n",
    "\n",
    "    # Combine to final train/test for this fold\n",
    "    train_idx = np.concatenate([gest_train_idx, fn_train_idx])\n",
    "    test_idx  = np.concatenate([gest_test_idx,  fn_test_idx])\n",
    "\n",
    "    res = train_eval_one_split(train_idx, test_idx, seed=seed, tag=f\"LOSO_user_{u}\")\n",
    "    results_loso.append((u, res))\n",
    "    print(f\"[LOSO {u}] acc={res['acc']:.3f}  f1_macro={res['f1_macro']:.3f}\")\n",
    "\n",
    "    save_dir = MODELS_UNIFIED_ROOT / \"LOSO\" / f\"user_{u}\"\n",
    "    save_unified_artifacts(save_dir, res[\"state\"], res[\"cfg\"],\n",
    "                           {\"acc\": res[\"acc\"], \"f1_macro\": res[\"f1_macro\"]},\n",
    "                           res[\"cm\"], CLASS_NAMES)\n",
    "\n",
    "print(\"LOSO avg acc:\", np.mean([r[1][\"acc\"] for r in results_loso]).round(3))\n",
    "print(\"LOSO avg f1 :\", np.mean([r[1][\"f1_macro\"] for r in results_loso]).round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6b0bef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mixed 1] acc=0.981  f1_macro=0.979\n",
      "[Mixed 2] acc=0.981  f1_macro=0.979\n",
      "[Mixed 3] acc=0.962  f1_macro=0.958\n",
      "[Mixed 4] acc=0.942  f1_macro=0.944\n",
      "[Mixed 5] acc=0.923  f1_macro=0.922\n",
      "Mixed avg acc: 0.958\n",
      "Mixed avg f1 : 0.956\n"
     ]
    }
   ],
   "source": [
    "# CELL 10 — Mixed 80/20 experiment\n",
    "# =========================\n",
    "results_mixed = []\n",
    "\n",
    "# Integer labels for stratification\n",
    "y_for_strat = np.array([LABEL_TO_IDX[l] for l in labels])\n",
    "\n",
    "for i in range(MIXED_SEEDS):\n",
    "    seed = 100 + i\n",
    "    seed_everything(seed)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=MIXED_TEST_SIZE, random_state=seed)\n",
    "    train_idx, test_idx = next(sss.split(all_idx, y_for_strat))\n",
    "\n",
    "    res = train_eval_one_split(train_idx, test_idx, seed=seed, tag=f\"Mixed_seed_{seed}\")\n",
    "    results_mixed.append(res)\n",
    "    print(f\"[Mixed {i+1}] acc={res['acc']:.3f}  f1_macro={res['f1_macro']:.3f}\")\n",
    "\n",
    "    save_dir = MODELS_UNIFIED_ROOT / \"Mixed\" / f\"seed_{seed}\"\n",
    "    save_unified_artifacts(save_dir, res[\"state\"], res[\"cfg\"],\n",
    "                           {\"acc\": res[\"acc\"], \"f1_macro\": res[\"f1_macro\"]},\n",
    "                           res[\"cm\"], CLASS_NAMES)\n",
    "\n",
    "print(\"Mixed avg acc:\", np.mean([r[\"acc\"] for r in results_mixed]).round(3))\n",
    "print(\"Mixed avg f1 :\", np.mean([r[\"f1_macro\"] for r in results_mixed]).round(3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
