{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2b7f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hamad\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleNetV2 x1.0 float @224:  5.7 ms\n",
      "MNASNet 1.0 @224:              6.0 ms\n",
      "EfficientNet-Lite0 @224:       6.1 ms\n",
      "EfficientNet-B1 @224:          10.8 ms\n",
      "EfficientNet-B7 @224:          47.0 ms\n",
      "EfficientNetV2-S @224:         22.7 ms\n",
      "EfficientNetV2-M @224:         40.3 ms\n"
     ]
    }
   ],
   "source": [
    "# --------- Benchmark different image classification models ----------\n",
    "import time, cv2, torch, timm\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "\n",
    "# ---------- system setup ----------\n",
    "torch.set_num_threads(4)                # Pi 5 has 4 cores\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# ---------- image & transforms ----------\n",
    "IMAGE_PATH = Path(\"Processed Dataset/ProcessedFallDetection/Falling/FDF1.png\")\n",
    "\n",
    "def make_input(img_size):\n",
    "    bgr = cv2.imread(str(IMAGE_PATH), cv2.IMREAD_COLOR)\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    tform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((img_size, img_size), antialias=True),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    x = tform(rgb).unsqueeze(0)  # [1,3,H,W]\n",
    "    return x\n",
    "\n",
    "def time_model(model, x, warmup=30, iters=100):\n",
    "    model.eval()\n",
    "    # warmup\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(x)\n",
    "    # measure\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(iters):\n",
    "            _ = model(x)\n",
    "    t1 = time.perf_counter()\n",
    "    return (t1 - t0) / iters * 1000.0  # ms / image\n",
    "\n",
    "# ---------- models ----------\n",
    "# 1) EfficientNet-Lite0 (float) as feature extractor (num_classes=0 gives pooled embedding)\n",
    "lite224 = timm.create_model(\"tf_efficientnet_lite0\", pretrained=True, num_classes=0, global_pool=\"avg\").to(device)\n",
    "x224 = make_input(224)\n",
    "lite224_ms = time_model(lite224, x224)\n",
    "\n",
    "# 2) ShuffleNetV2 x1.5 (float)\n",
    "from torchvision.models import shufflenet_v2_x1_5, ShuffleNet_V2_X1_5_Weights\n",
    "shuf_float = shufflenet_v2_x1_5(weights=ShuffleNet_V2_X1_5_Weights.DEFAULT)\n",
    "shuf_float_ms = time_model(shuf_float, x224)\n",
    "\n",
    "# 3) MNASNet 1_0\n",
    "from torchvision.models import mnasnet1_0, MNASNet1_0_Weights\n",
    "mnas = mnasnet1_0(weights=MNASNet1_0_Weights.IMAGENET1K_V1)\n",
    "mnas_ms = time_model(mnas, x224)\n",
    "\n",
    "# 4) efficientnetb7\n",
    "from torchvision.models import efficientnet_b7, EfficientNet_B7_Weights\n",
    "effb7 = efficientnet_b7(weights=EfficientNet_B7_Weights.DEFAULT)\n",
    "effb7_ms = time_model(effb7, x224)\n",
    "\n",
    "# 5) efficientnetb1\n",
    "from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
    "effb1 = efficientnet_b1(weights=EfficientNet_B1_Weights.DEFAULT)\n",
    "effb1_ms = time_model(effb1, x224)\n",
    "\n",
    "# 6) efficientnetv2 s\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "effv2s = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.DEFAULT)\n",
    "effv2s_ms = time_model(effv2s, x224)\n",
    "\n",
    "# 7) efficientnetv2 m\n",
    "from torchvision.models import efficientnet_v2_m, EfficientNet_V2_M_Weights\n",
    "effv2m = efficientnet_v2_m(weights=EfficientNet_V2_M_Weights.DEFAULT)\n",
    "effv2m_ms = time_model(effv2m, x224)\n",
    "\n",
    "\n",
    "# ---------- print results ----------\n",
    "print(f\"ShuffleNetV2 x1.0 float @224:  {shuf_float_ms:.1f} ms\")\n",
    "print(f\"MNASNet 1.0 @224:              {mnas_ms:.1f} ms\")\n",
    "print(f\"EfficientNet-Lite0 @224:       {lite224_ms:.1f} ms\")\n",
    "print(f\"EfficientNet-B1 @224:          {effb1_ms:.1f} ms\")\n",
    "print(f\"EfficientNet-B7 @224:          {effb7_ms:.1f} ms\")\n",
    "print(f\"EfficientNetV2-S @224:         {effv2s_ms:.1f} ms\")\n",
    "print(f\"EfficientNetV2-M @224:         {effv2m_ms:.1f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "814b7dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ 5.7420, -1.7450, -3.7541, -2.4983, -3.6492, -0.6266]]) \n",
      "\n",
      "Predicted class: fall  (confidence: 99.73%)\n",
      "Prediction time: 23.84 ms\n",
      "\n",
      "fall: 99.73%\n",
      "light: 0.06%\n",
      "fan: 0.01%\n",
      "curtain: 0.03%\n",
      "screen: 0.01%\n",
      "none: 0.17%\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Example inference on a single image -----------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import time\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "\n",
    "IMAGE_SIZE = 224  # input image size\n",
    "model_name = \"efficientnet_v2_s\"  # model architecture\n",
    "\n",
    "# 1. Define the same model architecture you used during training\n",
    "def make_backbone_and_dim(model_name: str = \"efficientnet_v2_s\"):\n",
    "    \"\"\"Create conv backbone and return (module, feature_channels). \"\"\"\n",
    "    if model_name == \"efficientnet_v2_s\":\n",
    "        m = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.DEFAULT)\n",
    "\n",
    "    backbone = m.features  # conv feature extractor (no classifier)\n",
    "    # Infer channel dim with a dummy forward at current IMAGE_SIZE\n",
    "    with torch.no_grad():\n",
    "        backbone.eval()\n",
    "        dummy = torch.zeros(1, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "        feats = backbone(dummy)\n",
    "        feat_dim = feats.shape[1]  # channels\n",
    "    return backbone, feat_dim\n",
    "\n",
    "class UnifiedNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple classifier:\n",
    "      - conv backbone (MobileNetV3-Large)\n",
    "      - global max pooling (keeps strong motion edges)\n",
    "      - BN + Dropout\n",
    "      - Linear -> ReLU -> Dropout -> Linear (to NUM_CLASSES)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_classes: int,\n",
    "                 model_name: str = \"efficientnet_v2_s\",\n",
    "                 freeze_backbone: bool = True,\n",
    "                 head_hidden: int = 128,\n",
    "                 dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.backbone, feat_dim = make_backbone_and_dim()\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.pool = nn.AdaptiveMaxPool2d(1)   # global max pooling\n",
    "        self.bn   = nn.BatchNorm1d(head_hidden)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc1  = nn.Linear(feat_dim, head_hidden)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2  = nn.Linear(head_hidden, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)            # [B, C, H, W]\n",
    "        x = self.pool(x).flatten(1)     # [B, C]\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n",
    "\n",
    "# 2. Load trained model\n",
    "model_path = Path(\"Models/Unified/Mixed/seed_101/model_state.pt\")  # update this if your filename is different\n",
    "model = UnifiedNet(num_classes=6)  # same num_classes as in training\n",
    "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "model.eval()\n",
    "\n",
    "# 3. Prepare test image\n",
    "image_path = \"Processed Dataset/ProcessedFallDetection/Falling/FDF1.png\"  # path to your test image\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "classes = [\"fall\", \"light\", \"fan\", \"curtain\", \"screen\", \"none\"]\n",
    "\n",
    "img = cv2.imread(image_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "x = transform(img).unsqueeze(0)  # add batch dimension\n",
    "\n",
    "# 4. Run inference and measure time\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    logits = model(x)\n",
    "    end = time.time()\n",
    "\n",
    "# 5. Print results\n",
    "print(\"Logits:\", logits, \"\\n\")\n",
    "\n",
    "# Convert logits -> probabilities\n",
    "probs = torch.softmax(logits, dim=1)  # [1, num_classes]\n",
    "\n",
    "# Top-1 prediction and confidence\n",
    "pred = torch.argmax(probs, dim=1).item()\n",
    "conf = probs[0, pred].item()\n",
    "print(f\"Predicted class: {classes[pred]}  (confidence: {conf*100:.2f}%)\")\n",
    "print(f\"Prediction time: {(end - start)*1000:.2f} ms\\n\")\n",
    "\n",
    "# All class probabilities\n",
    "for i, p in enumerate(probs[0]):\n",
    "    print(f\"{classes[i]}: {p.item()*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
