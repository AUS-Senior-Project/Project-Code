{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc2bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Install necessary packages ----------\n",
    "%pip install numpy pandas opencv-python scikit-learn matplotlib tqdm pillow timm pyyaml joblib \n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa8f3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hamad\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ---------- Import necessary libraries ----------\n",
    "import os, json, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models import efficientnet_b7, EfficientNet_B7_Weights, inception_v3, Inception_V3_Weights\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28b151d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 5070 Ti\n",
      "CUDA is available! Training on GPU...\n"
     ]
    }
   ],
   "source": [
    "# ---------- Device setup and check GPU availability ----------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA is available! Training on GPU...\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Training on CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b88da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Variables Declaration ----------\n",
    "ROOT = Path(\".\")  # Project-Code root\n",
    "CSV_FALL = ROOT / \"FallDetectionLabels.csv\"\n",
    "CSV_GEST = ROOT / \"HandGestureLabels.csv\"\n",
    "\n",
    "DATASET_DIR = ROOT / \"Dataset\"\n",
    "OUT_FALL = ROOT / \"Processed Dataset\" / \"ProcessedFallDetection\"\n",
    "OUT_GEST = ROOT / \"Processed Dataset\" / \"ProcessedHandGesture\"\n",
    "\n",
    "PROCESSED_CSV_FALL = ROOT / \"ProcessedFallDetection.csv\"\n",
    "PROCESSED_CSV_GEST = ROOT / \"ProcessedHandGesture.csv\"\n",
    "\n",
    "SAMPLE_EVERY = 1\n",
    "OUT_SIZE = 600  # EfficientNet-B7 scale\n",
    "OVERWRITE = False\n",
    "VIDEO_EXTS = [\".mp4\", \".avi\", \".mov\", \".mkv\", \".MP4\", \".AVI\", \".MOV\", \".MKV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d45855f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 4169.83it/s]\n",
      "100%|██████████| 160/160 [00:00<00:00, 4577.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Video preprocessing script to convert videos to motion images\n",
    "\n",
    "# ---------- utils ----------\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# resolve video path function\n",
    "def resolve_video_path(rel_path: str) -> Path:\n",
    "    \"\"\"Resolve a relative CSV path to an actual video file.\n",
    "    Accepts with or without extension, or a dir containing a single video.\"\"\"\n",
    "    p = ROOT / rel_path\n",
    "    if p.is_file():\n",
    "        return p\n",
    "\n",
    "    # try adding common extensions if no suffix\n",
    "    if p.suffix == \"\":\n",
    "        for ext in VIDEO_EXTS:\n",
    "            cand = p.with_suffix(ext)\n",
    "            if cand.is_file():\n",
    "                return cand\n",
    "\n",
    "    # if it's a directory, pick the first known video\n",
    "    if p.is_dir():\n",
    "        for ext in VIDEO_EXTS:\n",
    "            vids = sorted(p.glob(f\"*{ext}\"))\n",
    "            if vids:\n",
    "                return vids[0]\n",
    "\n",
    "    raise FileNotFoundError(f\"Video not found for entry: {rel_path}\")\n",
    "\n",
    "# pad image to square function\n",
    "def pad_to_square(img: np.ndarray) -> np.ndarray:\n",
    "    h, w = img.shape[:2]\n",
    "    s = max(h, w)\n",
    "    top = (s - h) // 2\n",
    "    bottom = s - h - top\n",
    "    left = (s - w) // 2\n",
    "    right = s - w - left\n",
    "    return cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=0)\n",
    "\n",
    "# main video processing function, video to motion image\n",
    "def process_video_to_motion_image(video_path: Path, sample_every: int = 5, out_size: int = 600) -> np.ndarray:\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Failed to open {video_path}\")\n",
    "\n",
    "    sampled = []\n",
    "    idx = 0\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "        if idx % sample_every == 0:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
    "            sampled.append(gray)\n",
    "        idx += 1\n",
    "    cap.release()\n",
    "\n",
    "    if len(sampled) < 2:\n",
    "        raise RuntimeError(f\"Not enough sampled frames ({len(sampled)}) in {video_path}\")\n",
    "\n",
    "    acc = np.zeros_like(sampled[0], dtype=np.float32)\n",
    "    prev = sampled[0]\n",
    "    for cur in sampled[1:]:\n",
    "        diff = np.abs(cur - prev)\n",
    "        thr = np.percentile(diff, 99) # threshold to reduce noise\n",
    "        mask = diff >= thr\n",
    "        acc[mask] += diff[mask]\n",
    "        prev = cur\n",
    "\n",
    "    m = acc.max()\n",
    "    if m > 0:\n",
    "        acc = acc / m\n",
    "    acc = (acc * 255.0).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    acc = pad_to_square(acc)\n",
    "    acc = cv2.resize(acc, (out_size, out_size), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # stack grayscale to 3 channels for EfficientNet\n",
    "    img3 = np.stack([acc, acc, acc], axis=2)\n",
    "    return img3\n",
    "\n",
    "# output path mapping function\n",
    "def target_path_for_output(out_root: Path, rel_video_path: str, ext: str = \".png\") -> Path:\n",
    "    \"\"\"Write directly under the processed root without an extra Dataset/HandGesture or Dataset/FallDetection level.\n",
    "    Example:\n",
    "      Dataset/HandGesture/CurtainGesture/Hamad/CGH1.mp4\n",
    "      -> OUT_GEST/CurtainGesture/Hamad/CGH1.png\n",
    "    \"\"\"\n",
    "    rel = Path(rel_video_path)\n",
    "    parts = list(rel.parts)\n",
    "\n",
    "    # strip leading 'Dataset'\n",
    "    if parts and parts[0].lower() == \"dataset\":\n",
    "        parts = parts[1:]\n",
    "\n",
    "    # also strip the next level if it is HandGesture or FallDetection\n",
    "    if parts and parts[0].lower() in (\"handgesture\", \"falldetection\"):\n",
    "        parts = parts[1:]\n",
    "\n",
    "    rel_no_ext = Path(*parts).with_suffix(ext)\n",
    "    return out_root / rel_no_ext\n",
    "\n",
    "# process a dataframe table\n",
    "def process_table(df: pd.DataFrame, out_root: Path, has_user: bool) -> pd.DataFrame:\n",
    "    records, failures = [], []\n",
    "\n",
    "    for row in tqdm(df.itertuples(index=False), total=len(df)):\n",
    "        rel_path = getattr(row, \"video_path\")\n",
    "        label = getattr(row, \"label\")\n",
    "        user_id = getattr(row, \"user_id\") if has_user else None\n",
    "\n",
    "        try:\n",
    "            abs_video = resolve_video_path(rel_path)\n",
    "            out_img = target_path_for_output(out_root, rel_path, ext=\".png\")\n",
    "            ensure_dir(out_img.parent)\n",
    "\n",
    "            if OVERWRITE or not out_img.exists():\n",
    "                img = process_video_to_motion_image(abs_video, sample_every=SAMPLE_EVERY, out_size=OUT_SIZE)\n",
    "                ok = cv2.imwrite(str(out_img), cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "                if not ok:\n",
    "                    raise RuntimeError(\"cv2.imwrite failed\")\n",
    "\n",
    "            rel_img = out_img.relative_to(ROOT).as_posix()\n",
    "            rec = {\"image_path\": rel_img, \"label\": label, \"video_path\": rel_path}\n",
    "            if has_user:\n",
    "                rec[\"user_id\"] = user_id\n",
    "            records.append(rec)\n",
    "\n",
    "        except Exception as e:\n",
    "            failures.append({\"video_path\": rel_path, \"label\": label, \"error\": str(e)})\n",
    "\n",
    "    if failures:\n",
    "        fail_csv = out_root / \"processing_failures.csv\"\n",
    "        pd.DataFrame(failures).to_csv(fail_csv, index=False)\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# ---------- run preprocessing ----------\n",
    "ensure_dir(OUT_FALL)\n",
    "ensure_dir(OUT_GEST)\n",
    "\n",
    "if CSV_FALL.exists():\n",
    "    df_fall = pd.read_csv(CSV_FALL)\n",
    "    out_fall_df = process_table(df_fall, OUT_FALL, has_user=False)\n",
    "    out_fall_df.to_csv(PROCESSED_CSV_FALL, index=False)\n",
    "else:\n",
    "    print(f\"Missing CSV: {CSV_FALL}\")\n",
    "\n",
    "if CSV_GEST.exists():\n",
    "    df_gest = pd.read_csv(CSV_GEST)\n",
    "    out_gest_df = process_table(df_gest, OUT_GEST, has_user=True)\n",
    "    out_gest_df.to_csv(PROCESSED_CSV_GEST, index=False)\n",
    "else:\n",
    "    print(f\"Missing CSV: {CSV_GEST}\")\n",
    "\n",
    "print(\"Preprocessing done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9478c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Variable Declaration ----------\n",
    "FEATURES_ROOT = ROOT / \"Features\" / \"EffB7\"\n",
    "OUT_FEAT_FALL_DIR = FEATURES_ROOT / \"FallDetection\"\n",
    "OUT_FEAT_GEST_DIR = FEATURES_ROOT / \"HandGesture\"\n",
    "MAP_CSV_FALL = ROOT / \"FallDetectionFeatures.csv\"\n",
    "MAP_CSV_GEST = ROOT / \"HandGestureFeatures.csv\"\n",
    "\n",
    "FEATURE_DIM = 2560\n",
    "BATCH_SIZE = 2\n",
    "NUM_WORKERS = 0\n",
    "OVERWRITE_FEATURES = False  # set False to skip existing\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ac3d74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]C:\\Users\\Hamad\\AppData\\Local\\Temp\\ipykernel_45908\\1662957103.py:71: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
      "100%|██████████| 50/50 [00:05<00:00,  9.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: FallDetectionFeatures.csv | Skipped existing: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/80 [00:00<?, ?it/s]C:\\Users\\Hamad\\AppData\\Local\\Temp\\ipykernel_45908\\1662957103.py:71: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
      "100%|██████████| 80/80 [00:02<00:00, 30.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: HandGestureFeatures.csv | Skipped existing: 160\n",
      "Feature extraction finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Feature extraction with EfficientNet-B7 (timm) ---\n",
    "\n",
    "# Model (separate name to avoid clashing with later training models)\n",
    "feat_model = timm.create_model(\"tf_efficientnet_b7\", pretrained=True, num_classes=0, global_pool=\"avg\")  # 2560-d\n",
    "feat_model.eval().to(device)\n",
    "\n",
    "# Images are already 600x600, 3ch; no resize here\n",
    "extract_tform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # HWC uint8 -> CHW float in [0,1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Processed Dataset class\n",
    "class ProcessedImageDataset(Dataset):\n",
    "    def __init__(self, csv_path: Path, root: Path, has_user: bool):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.root = root\n",
    "        self.has_user = has_user\n",
    "        self.has_user_col = \"user_id\" in self.df.columns\n",
    "        self.has_video_col = \"video_path\" in self.df.columns\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_rel = row[\"image_path\"]\n",
    "        img_abs = self.root / img_rel\n",
    "        bgr = cv2.imread(str(img_abs), cv2.IMREAD_COLOR)\n",
    "        if bgr is None:\n",
    "            raise FileNotFoundError(f\"Missing image: {img_abs}\")\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        x = extract_tform(rgb)\n",
    "\n",
    "        label = row[\"label\"]\n",
    "        user = row[\"user_id\"] if (self.has_user and self.has_user_col) else \"\"\n",
    "        vpath = row[\"video_path\"] if self.has_video_col else \"\"\n",
    "        return x, img_rel, label, user, vpath\n",
    "\n",
    "# path mapping function\n",
    "def feature_path_for_image(out_root: Path, image_rel_path: str) -> Path:\n",
    "    \"\"\"\n",
    "    Map processed image path to features root, removing:\n",
    "      'Processed Dataset' and the dataset-specific level.\n",
    "    \"\"\"\n",
    "    rel = Path(image_rel_path)\n",
    "    parts = list(rel.parts)\n",
    "    if parts and parts[0].lower() == \"processed dataset\":\n",
    "        parts = parts[1:]\n",
    "    if parts and parts[0].lower() in (\"processedhandgesture\", \"processedfalldetection\"):\n",
    "        parts = parts[1:]\n",
    "    rel_no_ext = Path(*parts).with_suffix(\".npy\")\n",
    "    return out_root / rel_no_ext\n",
    "\n",
    "# feature extraction function\n",
    "@torch.no_grad()\n",
    "def extract_and_save_features(csv_in: Path, out_dir: Path, map_csv_out: Path, has_user: bool):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    FEATURES_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ds = ProcessedImageDataset(csv_in, ROOT, has_user=has_user)\n",
    "    g = torch.Generator(device=\"cpu\"); g.manual_seed(SEED)\n",
    "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=True, generator=g)\n",
    "\n",
    "    records, skipped = [], 0\n",
    "\n",
    "    for xb, img_rels, labels, users, vpaths in tqdm(loader, total=len(loader)):\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
    "            feats = feat_model(xb)  # [B, 2560]\n",
    "        feats = feats.float().cpu().numpy()\n",
    "\n",
    "        for i in range(len(img_rels)):\n",
    "            img_rel = img_rels[i]\n",
    "            feat_path = feature_path_for_image(out_dir, img_rel)\n",
    "            feat_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            if not OVERWRITE_FEATURES and feat_path.exists():\n",
    "                skipped += 1\n",
    "            else:\n",
    "                np.save(feat_path, feats[i])\n",
    "\n",
    "            rec = {\n",
    "                \"image_path\": img_rel,\n",
    "                \"feature_path\": feat_path.relative_to(ROOT).as_posix(),\n",
    "                \"label\": labels[i],\n",
    "            }\n",
    "            if has_user:\n",
    "                rec[\"user_id\"] = users[i]\n",
    "            if vpaths[i]:\n",
    "                rec[\"video_path\"] = vpaths[i]\n",
    "            records.append(rec)\n",
    "\n",
    "    pd.DataFrame(records).to_csv(map_csv_out, index=False)\n",
    "    print(f\"Saved: {map_csv_out} | Skipped existing: {skipped}\")\n",
    "\n",
    "# ---- run both datasets (uses existing PROCESSED_CSV_* vars) ----\n",
    "if PROCESSED_CSV_FALL.exists():\n",
    "    extract_and_save_features(PROCESSED_CSV_FALL, OUT_FEAT_FALL_DIR, MAP_CSV_FALL, has_user=False)\n",
    "else:\n",
    "    print(f\"Missing {PROCESSED_CSV_FALL}\")\n",
    "\n",
    "if PROCESSED_CSV_GEST.exists():\n",
    "    extract_and_save_features(PROCESSED_CSV_GEST, OUT_FEAT_GEST_DIR, MAP_CSV_GEST, has_user=True)\n",
    "else:\n",
    "    print(f\"Missing {PROCESSED_CSV_GEST}\")\n",
    "\n",
    "print(\"Feature extraction finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d999595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in mapping: 100\n",
      "Missing files: 0\n",
      "Bad shapes: 0  expected (2560,)\n",
      "NaN or Inf vectors: 0\n",
      "Image: Processed Dataset/ProcessedFallDetection/Standing/FDSD4.png\n",
      "Label: fall\n",
      "Feature shape: (2560,)\n",
      "First 20 values: [ 0.0212  0.1653 -0.0696 -0.1597 -0.0088  0.1852 -0.021  -0.1089 -0.0632\n",
      " -0.1142 -0.0655  0.2129 -0.1145 -0.1143  0.0795  0.1097 -0.1205 -0.0143\n",
      " -0.0798  0.2352]\n",
      "Stats min max mean std: -0.2078857421875 0.54931640625 -0.008193041197955608 0.11091436445713043\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in mapping: 160\n",
      "Missing files: 0\n",
      "Bad shapes: 0  expected (2560,)\n",
      "NaN or Inf vectors: 0\n",
      "Image: Processed Dataset/ProcessedHandGesture/LightGesture/Obaid/LGO6.png\n",
      "Label: light\n",
      "User: obaid\n",
      "Feature shape: (2560,)\n",
      "First 20 values: [-0.0997  0.2111 -0.0464 -0.0949 -0.0263  0.0795 -0.1243  0.022   0.1597\n",
      " -0.2031 -0.1234  0.2179 -0.1169 -0.1703 -0.0372  0.0208 -0.0811 -0.1718\n",
      " -0.1069  0.4253]\n",
      "Stats min max mean std: -0.2235107421875 0.99658203125 -0.0018382944399490952 0.13646206259727478\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the mapping CSV and feature files for issues, and print a random example\n",
    "def check_features(map_csv):\n",
    "    df = pd.read_csv(map_csv)\n",
    "    print(f\"Rows in mapping: {len(df)}\")\n",
    "\n",
    "    missing = []\n",
    "    bad_shape = []\n",
    "    nan_inf = 0\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        fp = (ROOT / row[\"feature_path\"])\n",
    "        if not fp.exists():\n",
    "            missing.append(row[\"feature_path\"])\n",
    "            continue\n",
    "        v = np.load(fp)\n",
    "        if v.shape != (FEATURE_DIM,):\n",
    "            bad_shape.append((row[\"feature_path\"], v.shape))\n",
    "        if not np.isfinite(v).all():\n",
    "            nan_inf += 1\n",
    "\n",
    "    print(f\"Missing files: {len(missing)}\")\n",
    "    print(f\"Bad shapes: {len(bad_shape)}  expected {(FEATURE_DIM,)}\")\n",
    "    print(f\"NaN or Inf vectors: {nan_inf}\")\n",
    "\n",
    "    if missing:\n",
    "        print(\"Example missing:\", missing[:3])\n",
    "    if bad_shape:\n",
    "        print(\"Example bad shape:\", bad_shape[:3])\n",
    "\n",
    "\n",
    "\n",
    "    row = df.sample(1, random_state=SEED).iloc[0]  # random pick\n",
    "    fp = ROOT / row[\"feature_path\"]\n",
    "    v = np.load(fp)\n",
    "\n",
    "    print(\"Image:\", row[\"image_path\"])\n",
    "    print(\"Label:\", row[\"label\"])\n",
    "    if \"user_id\" in row:\n",
    "        print(\"User:\", row[\"user_id\"])\n",
    "    print(\"Feature shape:\", v.shape)\n",
    "    print(\"First 20 values:\", np.round(v[:20], 4))\n",
    "    print(\"Stats min max mean std:\", float(v.min()), float(v.max()), float(v.mean()), float(v.std()))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(v, bins=50)\n",
    "    plt.title(\"Feature value distribution\")\n",
    "    plt.xlabel(\"value\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.show()\n",
    "\n",
    "# Feature example check\n",
    "check_features(MAP_CSV_FALL)\n",
    "check_features(MAP_CSV_GEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5bc6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Variables Declaration ----------\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "HIDDEN_DIM = 512\n",
    "DROPOUT = 0.3\n",
    "APPLY_STANDARDIZE = True     # fit on train only, apply to val/test\n",
    "APPLY_L2 = False             # optional per-sample L2 normalize\n",
    "MODELS_ROOT = ROOT / \"Models\"  # save trained models here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c364a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Training and evaluation of MLP classifier on extracted features ----------\n",
    "\n",
    "# ---- Repro utilities (use existing SEED; no reinit of device) ----\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ---- Data loading from mapping CSV ----\n",
    "def load_features_from_map(map_csv: Path, has_user: bool):\n",
    "    df = pd.read_csv(map_csv)\n",
    "    X = np.stack([np.load((ROOT / fp).as_posix()) for fp in df[\"feature_path\"]]).astype(np.float32)\n",
    "    y_text = df[\"label\"].astype(str).values\n",
    "    enc = LabelEncoder()\n",
    "    y = enc.fit_transform(y_text)\n",
    "    meta = {\"labels_text\": y_text, \"label_encoder\": enc}\n",
    "    if has_user and \"user_id\" in df.columns:\n",
    "        meta[\"user_id\"] = df[\"user_id\"].astype(str).values\n",
    "    meta[\"image_path\"] = df[\"image_path\"].astype(str).values\n",
    "    return X, y, meta\n",
    "\n",
    "# Dataset class for loading numpy features\n",
    "class NumpyFeatDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y).long()\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
    "\n",
    "# Simple MLP classifier\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, HIDDEN_DIM),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(HIDDEN_DIM, num_classes),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "# Apply scaling (standardization + L2 normalization)\n",
    "def apply_scaling(X_train, X_val, X_test):\n",
    "    scaler = None\n",
    "    if APPLY_STANDARDIZE:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    if APPLY_L2:\n",
    "        X_train = X_train / (np.linalg.norm(X_train, axis=1, keepdims=True) + 1e-8)\n",
    "        X_val   = X_val   / (np.linalg.norm(X_val,   axis=1, keepdims=True) + 1e-8)\n",
    "        X_test  = X_test  / (np.linalg.norm(X_test,  axis=1, keepdims=True) + 1e-8)\n",
    "    return X_train.astype(np.float32), X_val.astype(np.float32), X_test.astype(np.float32), scaler\n",
    "\n",
    "# Train one split (train/val) and return best metrics + state\n",
    "def train_one_split(X_train, y_train, X_val, y_val, num_classes, seed):\n",
    "    # reuse earlier helpers/vars: seed_everything, apply_scaling, FEATURE_DIM, device,\n",
    "    # TRAIN_BATCH_SIZE, EPOCHS, LR, WEIGHT_DECAY, DROPOUT, HIDDEN_DIM\n",
    "    seed_everything(seed)\n",
    "\n",
    "    X_train_s, X_val_s, X_val_for_eval, scaler = apply_scaling(X_train, X_val, X_val.copy())\n",
    "\n",
    "    train_ds = NumpyFeatDataset(X_train_s, y_train)\n",
    "    val_ds   = NumpyFeatDataset(X_val_s,   y_val)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=0\n",
    "    )\n",
    "    val_loader   = torch.utils.data.DataLoader(\n",
    "        val_ds,   batch_size=TRAIN_BATCH_SIZE, shuffle=False, num_workers=0\n",
    "    )\n",
    "\n",
    "    model = MLP(FEATURE_DIM, num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    best = {\"acc\": 0.0, \"state\": None}\n",
    "\n",
    "    for _ in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "\n",
    "        # quick val\n",
    "        model.eval()\n",
    "        all_logits, all_y = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                all_logits.append(model(xb).cpu())\n",
    "                all_y.append(yb)\n",
    "        logits = torch.cat(all_logits); y_true = torch.cat(all_y)\n",
    "        y_pred = logits.argmax(1)\n",
    "        acc = (y_pred == y_true).float().mean().item()\n",
    "        if acc > best[\"acc\"]:\n",
    "            best[\"acc\"] = acc\n",
    "            best[\"state\"] = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "    # final metrics with best\n",
    "    model.load_state_dict(best[\"state\"])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.from_numpy(X_val_s).to(device)).cpu().numpy()\n",
    "    y_pred = logits.argmax(1)\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1m = f1_score(y_val, y_pred, average=\"macro\")\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1_macro\": f1m,\n",
    "        \"cm\": cm,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"state\": best[\"state\"],\n",
    "        \"scaler\": scaler,\n",
    "    }\n",
    "\n",
    "# Save all artifacts from one run\n",
    "def save_run_artifacts(save_dir: Path,\n",
    "                       state_dict: dict,\n",
    "                       scaler,\n",
    "                       label_encoder,\n",
    "                       config: dict,\n",
    "                       metrics: dict,\n",
    "                       cm: np.ndarray,\n",
    "                       class_names: list):\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # model\n",
    "    torch.save(state_dict, save_dir / \"model_state.pt\")\n",
    "\n",
    "    # scaler + label encoder\n",
    "    with open(save_dir / \"scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    with open(save_dir / \"label_encoder.pkl\", \"wb\") as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "\n",
    "    # config + metrics\n",
    "    with open(save_dir / \"config.json\", \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    with open(save_dir / \"metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    # confusion matrix (csv + png)\n",
    "    np.savetxt(save_dir / \"confusion_matrix.csv\", cm, fmt=\"%d\", delimiter=\",\")\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    ax.set_xticks(range(len(class_names))); ax.set_xticklabels(class_names, rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(range(len(class_names))); ax.set_yticklabels(class_names)\n",
    "    # annotate\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, int(cm[i, j]), ha=\"center\", va=\"center\", fontsize=8)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_dir / \"confusion_matrix.png\", dpi=160)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6710f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fall] Run 1: acc=0.900  f1_macro=0.899\n",
      "[Fall] Run 2: acc=1.000  f1_macro=1.000\n",
      "[Fall] Run 3: acc=0.900  f1_macro=0.896\n",
      "[Fall] Run 4: acc=1.000  f1_macro=1.000\n",
      "[Fall] Run 5: acc=1.000  f1_macro=1.000\n",
      "Fall avg acc: 0.96\n",
      "Fall avg f1 : 0.959\n"
     ]
    }
   ],
   "source": [
    "# Train and Test Fall 80/20 across 5 seeds\n",
    "N_SEEDS = 5\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "X_fall, y_fall, meta_fall = load_features_from_map(MAP_CSV_FALL, has_user=False)\n",
    "class_names_fall = meta_fall[\"label_encoder\"].classes_.tolist()\n",
    "num_classes_fall = len(class_names_fall)\n",
    "\n",
    "results_fall = []\n",
    "for i, seed in enumerate(range(SEED, SEED + N_SEEDS), 1):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=seed)\n",
    "    train_idx, val_idx = next(sss.split(X_fall, y_fall))\n",
    "\n",
    "    res = train_one_split(\n",
    "        X_fall[train_idx], y_fall[train_idx],\n",
    "        X_fall[val_idx],   y_fall[val_idx],\n",
    "        num_classes_fall, seed\n",
    "    )\n",
    "    results_fall.append(res)\n",
    "    print(f\"[Fall] Run {i}: acc={res['acc']:.3f}  f1_macro={res['f1_macro']:.3f}\")\n",
    "\n",
    "    # save\n",
    "    save_dir = MODELS_ROOT / \"Fall\" / f\"seed_{seed}\"\n",
    "    cfg = {\n",
    "        \"dataset\": \"Fall\",\n",
    "        \"seed\": seed,\n",
    "        \"feature_dim\": FEATURE_DIM,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"lr\": LR,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"apply_standardize\": APPLY_STANDARDIZE,\n",
    "        \"apply_l2\": APPLY_L2,\n",
    "        \"classes\": class_names_fall,\n",
    "    }\n",
    "    metrics = {\"acc\": float(res[\"acc\"]), \"f1_macro\": float(res[\"f1_macro\"])}\n",
    "    save_run_artifacts(save_dir, res[\"state\"], res[\"scaler\"],\n",
    "                       meta_fall[\"label_encoder\"], cfg, metrics, res[\"cm\"], class_names_fall)\n",
    "\n",
    "print(\"Fall avg acc:\", np.mean([r[\"acc\"] for r in results_fall]).round(3))\n",
    "print(\"Fall avg f1 :\", np.mean([r[\"f1_macro\"] for r in results_fall]).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39f22037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gesture LOSO] Holdout hamad: acc=0.950  f1_macro=0.949\n",
      "[Gesture LOSO] Holdout mohammad: acc=1.000  f1_macro=1.000\n",
      "[Gesture LOSO] Holdout obaid: acc=0.900  f1_macro=0.900\n",
      "[Gesture LOSO] Holdout saif: acc=0.975  f1_macro=0.975\n",
      "Gesture LOSO avg acc: 0.956\n",
      "Gesture LOSO avg f1 : 0.956\n"
     ]
    }
   ],
   "source": [
    "# Train and Test Gesture LOSO (train on 3 users, test on 1)\n",
    "X_gest, y_gest, meta_gest = load_features_from_map(MAP_CSV_GEST, has_user=True)\n",
    "users = np.array(meta_gest[\"user_id\"])\n",
    "class_names_gest = meta_gest[\"label_encoder\"].classes_.tolist()\n",
    "num_classes_gest = len(class_names_gest)\n",
    "\n",
    "unique_users = sorted(np.unique(users))\n",
    "results_gest_loso = []\n",
    "\n",
    "for u in unique_users:\n",
    "    test_mask = (users == u)\n",
    "    train_mask = ~test_mask\n",
    "\n",
    "    res = train_one_split(\n",
    "        X_gest[train_mask], y_gest[train_mask],\n",
    "        X_gest[test_mask],  y_gest[test_mask],\n",
    "        num_classes_gest, seed=SEED\n",
    "    )\n",
    "    results_gest_loso.append((u, res))\n",
    "    print(f\"[Gesture LOSO] Holdout {u}: acc={res['acc']:.3f}  f1_macro={res['f1_macro']:.3f}\")\n",
    "\n",
    "    save_dir = MODELS_ROOT / \"Gesture_LOSO\" / f\"user_{u}\"\n",
    "    cfg = {\n",
    "        \"dataset\": \"Gesture_LOSO\",\n",
    "        \"holdout_user\": u,\n",
    "        \"seed\": SEED,\n",
    "        \"feature_dim\": FEATURE_DIM,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"lr\": LR,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"apply_standardize\": APPLY_STANDARDIZE,\n",
    "        \"apply_l2\": APPLY_L2,\n",
    "        \"classes\": class_names_gest,\n",
    "    }\n",
    "    metrics = {\"acc\": float(res[\"acc\"]), \"f1_macro\": float(res[\"f1_macro\"])}\n",
    "    save_run_artifacts(save_dir, res[\"state\"], res[\"scaler\"],\n",
    "                       meta_gest[\"label_encoder\"], cfg, metrics, res[\"cm\"], class_names_gest)\n",
    "\n",
    "print(\"Gesture LOSO avg acc:\", np.mean([r[1]['acc'] for r in results_gest_loso]).round(3))\n",
    "print(\"Gesture LOSO avg f1 :\", np.mean([r[1]['f1_macro'] for r in results_gest_loso]).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c527e597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gesture Mixed] Run 1: acc=1.000  f1_macro=1.000\n",
      "[Gesture Mixed] Run 2: acc=1.000  f1_macro=1.000\n",
      "[Gesture Mixed] Run 3: acc=1.000  f1_macro=1.000\n",
      "[Gesture Mixed] Run 4: acc=1.000  f1_macro=1.000\n",
      "[Gesture Mixed] Run 5: acc=0.969  f1_macro=0.969\n",
      "Gesture Mixed avg acc: 0.994\n",
      "Gesture Mixed avg f1 : 0.994\n"
     ]
    }
   ],
   "source": [
    "# Train and Test Gesture 80/20 across 5 seeds (mixed users)\n",
    "N_SEEDS = 5\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "results_gest_mix = []\n",
    "for i, seed in enumerate(range(SEED, SEED + N_SEEDS), 1):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=seed)\n",
    "    train_idx, val_idx = next(sss.split(X_gest, y_gest))\n",
    "\n",
    "    res = train_one_split(\n",
    "        X_gest[train_idx], y_gest[train_idx],\n",
    "        X_gest[val_idx],   y_gest[val_idx],\n",
    "        num_classes_gest, seed\n",
    "    )\n",
    "    results_gest_mix.append(res)\n",
    "    print(f\"[Gesture Mixed] Run {i}: acc={res['acc']:.3f}  f1_macro={res['f1_macro']:.3f}\")\n",
    "\n",
    "    save_dir = MODELS_ROOT / \"Gesture_Mixed\" / f\"seed_{seed}\"\n",
    "    cfg = {\n",
    "        \"dataset\": \"Gesture_Mixed\",\n",
    "        \"seed\": seed,\n",
    "        \"feature_dim\": FEATURE_DIM,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"lr\": LR,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"apply_standardize\": APPLY_STANDARDIZE,\n",
    "        \"apply_l2\": APPLY_L2,\n",
    "        \"classes\": class_names_gest,\n",
    "    }\n",
    "    metrics = {\"acc\": float(res[\"acc\"]), \"f1_macro\": float(res[\"f1_macro\"])}\n",
    "    save_run_artifacts(save_dir, res[\"state\"], res[\"scaler\"],\n",
    "                       meta_gest[\"label_encoder\"], cfg, metrics, res[\"cm\"], class_names_gest)\n",
    "\n",
    "print(\"Gesture Mixed avg acc:\", np.mean([r[\"acc\"] for r in results_gest_mix]).round(3))\n",
    "print(\"Gesture Mixed avg f1 :\", np.mean([r[\"f1_macro\"] for r in results_gest_mix]).round(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
